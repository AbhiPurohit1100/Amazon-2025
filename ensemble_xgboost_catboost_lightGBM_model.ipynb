{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d284acaa-e37e-4bff-b0ca-308667a6c0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1qsBYi-_LjeTTuEISItU_VFbFvtKEUA7T\n",
      "From (redirected): https://drive.google.com/uc?id=1qsBYi-_LjeTTuEISItU_VFbFvtKEUA7T&confirm=t&uuid=639843b7-8414-4b72-b696-bacfc3c7f085\n",
      "To: /teamspace/studios/this_studio/train_all.csv\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.41G/3.41G [00:15<00:00, 215MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'train_all.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "file_id = '1qsBYi-_LjeTTuEISItU_VFbFvtKEUA7T' # Replace with the actual file ID\n",
    "output_path = 'train_all.csv' # Specify your desired output filename\n",
    "\n",
    "gdown.download(f'https://drive.google.com/uc?id={file_id}', output_path, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e559c2f-d07c-4388-8705-550ea13c300d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1JfYlSgYO30YkNHn9qOhdPkfYDrhsK6uG\n",
      "From (redirected): https://drive.google.com/uc?id=1JfYlSgYO30YkNHn9qOhdPkfYDrhsK6uG&confirm=t&uuid=709bb3ae-bdb2-4824-b070-e7962218cd8c\n",
      "To: /teamspace/studios/this_studio/test_all.csv\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.41G/3.41G [00:31<00:00, 107MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test_all.csv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_id = '1JfYlSgYO30YkNHn9qOhdPkfYDrhsK6uG' # Replace with the actual file ID\n",
    "output_path = 'test_all.csv' # Specify your desired output filename\n",
    "gdown.download(f'https://drive.google.com/uc?id={file_id}', output_path, quiet=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "628f3b6c-9961-40a4-aea4-12b012c0240c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>title</th>\n",
       "      <th>value_raw</th>\n",
       "      <th>unit_raw</th>\n",
       "      <th>unit_norm</th>\n",
       "      <th>pack_count</th>\n",
       "      <th>is_multipack</th>\n",
       "      <th>total_qty_ml</th>\n",
       "      <th>unit_qty_ml</th>\n",
       "      <th>total_qty_g</th>\n",
       "      <th>...</th>\n",
       "      <th>container_sachet</th>\n",
       "      <th>container_pod</th>\n",
       "      <th>container_k-cup</th>\n",
       "      <th>container_k_cup</th>\n",
       "      <th>container_tin</th>\n",
       "      <th>catalog_content</th>\n",
       "      <th>image_link</th>\n",
       "      <th>text_finetuned_embeddings</th>\n",
       "      <th>title_embeddings</th>\n",
       "      <th>image_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100179</td>\n",
       "      <td>Rani 14-Spice Eshamaya's Mango Chutney (Indian...</td>\n",
       "      <td>10.5</td>\n",
       "      <td>ounce</td>\n",
       "      <td>g</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Item Name: Rani 14-Spice Eshamaya's Mango Chut...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71hoAn78AW...</td>\n",
       "      <td>[-0.12731970846652985, -0.7352365851402283, 0....</td>\n",
       "      <td>[0.08788865059614182, -0.39864382147789, -0.02...</td>\n",
       "      <td>[-8.83960910e-03  4.70816754e-02 -5.26126884e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245611</td>\n",
       "      <td>Natural MILK TEA Flavoring extract by HALO PAN...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>fl oz</td>\n",
       "      <td>ml</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Item Name: Natural MILK TEA Flavoring extract ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/61ex8NHCIj...</td>\n",
       "      <td>[0.6202144026756287, -0.28948888182640076, 0.5...</td>\n",
       "      <td>[-0.14431503415107727, -0.5547536611557007, -0...</td>\n",
       "      <td>[-1.80894732e-02  4.43181433e-02  1.79245938e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146263</td>\n",
       "      <td>Honey Filled Hard Candy - Bulk Pack 2 Pounds -...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>ounce</td>\n",
       "      <td>g</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Item Name: Honey Filled Hard Candy - Bulk Pack...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/61KCM61J8e...</td>\n",
       "      <td>[0.07451328635215759, -1.071375846862793, 1.15...</td>\n",
       "      <td>[0.3142585754394531, 1.1969234943389893, 0.032...</td>\n",
       "      <td>[-4.61538695e-02 -5.93220107e-02  3.14117788e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95658</td>\n",
       "      <td>Vlasic Snack'mm's Kosher Dill 16 Oz (Pack of 2)</td>\n",
       "      <td>2.0</td>\n",
       "      <td>count</td>\n",
       "      <td>count</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Item Name: Vlasic Snack'mm's Kosher Dill 16 Oz...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51Ex6uOH7y...</td>\n",
       "      <td>[0.18332837522029877, -1.6916332244873047, 0.6...</td>\n",
       "      <td>[0.11562508344650269, -1.4939545392990112, -1....</td>\n",
       "      <td>[-2.28938796e-02 -2.10853778e-02 -3.31543274e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36806</td>\n",
       "      <td>McCormick Culinary Vanilla Extract, 32 fl oz -...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>fl oz</td>\n",
       "      <td>ml</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Item Name: McCormick Culinary Vanilla Extract,...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71QYlrOMoS...</td>\n",
       "      <td>[-0.1778203248977661, -1.2165040969848633, 0.0...</td>\n",
       "      <td>[-0.7983670234680176, 0.4460751712322235, -0.0...</td>\n",
       "      <td>[-5.02053127e-02  2.41597686e-02  7.75490562e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id                                              title  value_raw  \\\n",
       "0     100179  Rani 14-Spice Eshamaya's Mango Chutney (Indian...       10.5   \n",
       "1     245611  Natural MILK TEA Flavoring extract by HALO PAN...        2.0   \n",
       "2     146263  Honey Filled Hard Candy - Bulk Pack 2 Pounds -...       32.0   \n",
       "3      95658    Vlasic Snack'mm's Kosher Dill 16 Oz (Pack of 2)        2.0   \n",
       "4      36806  McCormick Culinary Vanilla Extract, 32 fl oz -...       32.0   \n",
       "\n",
       "  unit_raw unit_norm  pack_count  is_multipack  total_qty_ml  unit_qty_ml  \\\n",
       "0    ounce         g           1             0           0.0          0.0   \n",
       "1    fl oz        ml           1             0           2.0          2.0   \n",
       "2    ounce         g           2             1           0.0          0.0   \n",
       "3    count     count           2             1           0.0          0.0   \n",
       "4    fl oz        ml           1             0          32.0         32.0   \n",
       "\n",
       "   total_qty_g  ...  container_sachet  container_pod  container_k-cup  \\\n",
       "0         10.5  ...                 0              0                0   \n",
       "1          0.0  ...                 0              0                0   \n",
       "2         32.0  ...                 0              0                0   \n",
       "3          0.0  ...                 0              0                0   \n",
       "4          0.0  ...                 0              0                0   \n",
       "\n",
       "  container_k_cup container_tin  \\\n",
       "0               0             0   \n",
       "1               0             0   \n",
       "2               0             0   \n",
       "3               0             0   \n",
       "4               0             0   \n",
       "\n",
       "                                     catalog_content  \\\n",
       "0  Item Name: Rani 14-Spice Eshamaya's Mango Chut...   \n",
       "1  Item Name: Natural MILK TEA Flavoring extract ...   \n",
       "2  Item Name: Honey Filled Hard Candy - Bulk Pack...   \n",
       "3  Item Name: Vlasic Snack'mm's Kosher Dill 16 Oz...   \n",
       "4  Item Name: McCormick Culinary Vanilla Extract,...   \n",
       "\n",
       "                                          image_link  \\\n",
       "0  https://m.media-amazon.com/images/I/71hoAn78AW...   \n",
       "1  https://m.media-amazon.com/images/I/61ex8NHCIj...   \n",
       "2  https://m.media-amazon.com/images/I/61KCM61J8e...   \n",
       "3  https://m.media-amazon.com/images/I/51Ex6uOH7y...   \n",
       "4  https://m.media-amazon.com/images/I/71QYlrOMoS...   \n",
       "\n",
       "                           text_finetuned_embeddings  \\\n",
       "0  [-0.12731970846652985, -0.7352365851402283, 0....   \n",
       "1  [0.6202144026756287, -0.28948888182640076, 0.5...   \n",
       "2  [0.07451328635215759, -1.071375846862793, 1.15...   \n",
       "3  [0.18332837522029877, -1.6916332244873047, 0.6...   \n",
       "4  [-0.1778203248977661, -1.2165040969848633, 0.0...   \n",
       "\n",
       "                                    title_embeddings  \\\n",
       "0  [0.08788865059614182, -0.39864382147789, -0.02...   \n",
       "1  [-0.14431503415107727, -0.5547536611557007, -0...   \n",
       "2  [0.3142585754394531, 1.1969234943389893, 0.032...   \n",
       "3  [0.11562508344650269, -1.4939545392990112, -1....   \n",
       "4  [-0.7983670234680176, 0.4460751712322235, -0.0...   \n",
       "\n",
       "                                     image_embedding  \n",
       "0  [-8.83960910e-03  4.70816754e-02 -5.26126884e-...  \n",
       "1  [-1.80894732e-02  4.43181433e-02  1.79245938e-...  \n",
       "2  [-4.61538695e-02 -5.93220107e-02  3.14117788e-...  \n",
       "3  [-2.28938796e-02 -2.10853778e-02 -3.31543274e-...  \n",
       "4  [-5.02053127e-02  2.41597686e-02  7.75490562e-...  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"test_all.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ca6465-1a18-4b04-9969-864284cdab70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: lightgbm in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: xgboost in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (3.0.5)\n",
      "Requirement already satisfied: catboost in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.2.8)\n",
      "Requirement already satisfied: category_encoders in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2.8.1)\n",
      "Requirement already satisfied: optuna in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.5.0)\n",
      "Requirement already satisfied: torch in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: torchvision in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.23.0+cu128)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from xgboost) (2.27.3)\n",
      "Requirement already satisfied: graphviz in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from catboost) (0.21)\n",
      "Requirement already satisfied: matplotlib in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from catboost) (3.8.2)\n",
      "Requirement already satisfied: plotly in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from catboost) (6.3.1)\n",
      "Requirement already satisfied: six in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from category_encoders) (1.0.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from category_encoders) (0.14.5)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from optuna) (1.17.0)\n",
      "Requirement already satisfied: colorlog in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from optuna) (2.0.44)\n",
      "Requirement already satisfied: tqdm in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from optuna) (6.0.3)\n",
      "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: Mako in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: greenlet>=1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib->catboost) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib->catboost) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib->catboost) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib->catboost) (3.2.5)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from plotly->catboost) (2.7.0)\n"
     ]
    }
   ],
   "source": [
    "# Install all required packages\n",
    "!pip install pandas numpy scikit-learn lightgbm xgboost catboost category_encoders optuna torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d5cefe8-beab-47f7-a321-981e95ea965a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸš€ COMPLETE ENSEMBLE PIPELINE\n",
      "================================================================================\n",
      "\n",
      "âœ“ GPU Detected: NVIDIA L40S\n",
      "  Memory: 47.9 GB\n",
      "  CUDA Version: 12.8\n",
      "  âœ“ A100 optimizations enabled\n",
      "\n",
      "âœ“ Configuration:\n",
      "  Device: cuda\n",
      "  Batch size: 512\n",
      "  Random state: 42\n",
      "  Test split: 0.2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PHASE 1 - CHUNK 2: Imports and A100 Configuration\n",
    "Complete Ensemble Pipeline for <41% SMAPE\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# PyTorch for Neural Network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸš€ COMPLETE ENSEMBLE PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# A100 GPU CONFIGURATION\n",
    "# ============================================================================\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nâœ“ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"  CUDA Version: {torch.version.cuda}\")\n",
    "    \n",
    "    # A100 Optimizations\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    print(f\"  âœ“ A100 optimizations enabled\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ CPU mode (slower)\")\n",
    "\n",
    "# ============================================================================\n",
    "# GLOBAL CONFIGURATION\n",
    "# ============================================================================\n",
    "TRAIN_FILE = 'train_all.csv'  # â† Change to your file\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# A100-optimized settings\n",
    "BATCH_SIZE = 512  # Large batch for A100\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY = True\n",
    "\n",
    "print(f\"\\nâœ“ Configuration:\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Random state: {RANDOM_STATE}\")\n",
    "print(f\"  Test split: {TEST_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6706fb7-8085-4fe5-8de9-60caefb36a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "âœ“ Loaded 75000 samples\n",
      "  Columns: 37\n",
      "\n",
      "ðŸ“Š Price Statistics:\n",
      "  Min: $0.13\n",
      "  Max: $2796.00\n",
      "  Mean: $23.65\n",
      "  Median: $14.00\n",
      "\n",
      "âœ“ Available columns:\n",
      "  - sample_id\n",
      "  - title\n",
      "  - value_raw\n",
      "  - unit_raw\n",
      "  - unit_norm\n",
      "  - pack_count\n",
      "  - is_multipack\n",
      "  - total_qty_ml\n",
      "  - unit_qty_ml\n",
      "  - total_qty_g\n",
      "  - unit_qty_g\n",
      "  - total_qty_count\n",
      "  - unit_qty_count\n",
      "  - brand\n",
      "  - brand_topk\n",
      "  - title_len_tokens\n",
      "  - digits_in_title\n",
      "  - is_organic_like\n",
      "  - is_bundle\n",
      "  - multi_flavor\n",
      "  - bucket4\n",
      "  - container_bottle\n",
      "  - container_jar\n",
      "  - container_can\n",
      "  - container_box\n",
      "  - container_pouch\n",
      "  - container_sachet\n",
      "  - container_pod\n",
      "  - container_k-cup\n",
      "  - container_k_cup\n",
      "  - container_tin\n",
      "  - catalog_content\n",
      "  - image_link\n",
      "  - price\n",
      "  - text_finetuned_embeddings\n",
      "  - title_embeddings\n",
      "  - image_embedding\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PHASE 1 - CHUNK 3: Load Training Data\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(TRAIN_FILE)\n",
    "\n",
    "print(f\"\\nâœ“ Loaded {len(train_df)} samples\")\n",
    "print(f\"  Columns: {len(train_df.columns)}\")\n",
    "\n",
    "# Check for price column\n",
    "if 'price' not in train_df.columns:\n",
    "    raise ValueError(\"âŒ No 'price' column found!\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Price Statistics:\")\n",
    "print(f\"  Min: ${train_df['price'].min():.2f}\")\n",
    "print(f\"  Max: ${train_df['price'].max():.2f}\")\n",
    "print(f\"  Mean: ${train_df['price'].mean():.2f}\")\n",
    "print(f\"  Median: ${train_df['price'].median():.2f}\")\n",
    "\n",
    "# Check available columns\n",
    "available_cols = train_df.columns.tolist()\n",
    "print(f\"\\nâœ“ Available columns:\")\n",
    "for col in available_cols:\n",
    "    print(f\"  - {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c05a26e-b24e-44dd-aee1-44c2211af93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PARSING EMBEDDINGS\n",
      "================================================================================\n",
      "\n",
      "[1/3] Parsing text_finetuned_embeddings...\n",
      "  âœ“ Shape: (75000, 1024)\n",
      "    Min: -3.0822, Max: 4.0148\n",
      "    Mean: 0.0127, Std: 0.6870\n",
      "\n",
      "[2/3] Parsing title_embeddings...\n",
      "  âœ“ Shape: (75000, 512)\n",
      "    Min: -14.8205, Max: 15.2988\n",
      "    Mean: 0.1191, Std: 0.7509\n",
      "\n",
      "[3/3] Parsing image_embedding...\n",
      "  âœ“ Shape: (75000, 768)\n",
      "    Min: -0.7196, Max: 0.3442\n",
      "    Mean: -0.0004, Std: 0.0361\n",
      "\n",
      "âœ“ Embedding Quality Checks:\n",
      "  text    : NaN=False, Inf=False, Variance=0.471908\n",
      "  title   : NaN=False, Inf=False, Variance=0.563855\n",
      "  image   : NaN=False, Inf=False, Variance=0.001302\n",
      "\n",
      "âœ“ Total embedding features: 2304\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PHASE 1 - CHUNK 4: Parse ALL Embedding Columns\n",
    "Text + Title + Image embeddings\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARSING EMBEDDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def parse_embedding(emb_str):\n",
    "    \"\"\"Convert string/list/array to numpy array\"\"\"\n",
    "    if isinstance(emb_str, str):\n",
    "        try:\n",
    "            emb = ast.literal_eval(emb_str)\n",
    "            return np.array(emb, dtype=np.float32)\n",
    "        except:\n",
    "            # Fallback parsing\n",
    "            emb = emb_str.replace('[', '').replace(']', '').replace(',', ' ').split()\n",
    "            return np.array([float(x) for x in emb if x], dtype=np.float32)\n",
    "    elif isinstance(emb_str, np.ndarray):\n",
    "        return emb_str.astype(np.float32)\n",
    "    elif isinstance(emb_str, list):\n",
    "        return np.array(emb_str, dtype=np.float32)\n",
    "    else:\n",
    "        return np.zeros(1, dtype=np.float32)\n",
    "\n",
    "# Parse text embeddings\n",
    "print(\"\\n[1/3] Parsing text_finetuned_embeddings...\")\n",
    "train_df['text_finetuned_embeddings'] = train_df['text_finetuned_embeddings'].apply(parse_embedding)\n",
    "text_matrix = np.stack(train_df['text_finetuned_embeddings'].values)\n",
    "print(f\"  âœ“ Shape: {text_matrix.shape}\")\n",
    "print(f\"    Min: {text_matrix.min():.4f}, Max: {text_matrix.max():.4f}\")\n",
    "print(f\"    Mean: {text_matrix.mean():.4f}, Std: {text_matrix.std():.4f}\")\n",
    "\n",
    "# Parse title embeddings\n",
    "print(\"\\n[2/3] Parsing title_embeddings...\")\n",
    "train_df['title_embeddings'] = train_df['title_embeddings'].apply(parse_embedding)\n",
    "title_matrix = np.stack(train_df['title_embeddings'].values)\n",
    "print(f\"  âœ“ Shape: {title_matrix.shape}\")\n",
    "print(f\"    Min: {title_matrix.min():.4f}, Max: {title_matrix.max():.4f}\")\n",
    "print(f\"    Mean: {title_matrix.mean():.4f}, Std: {title_matrix.std():.4f}\")\n",
    "\n",
    "# Parse image embeddings\n",
    "print(\"\\n[3/3] Parsing image_embedding...\")\n",
    "train_df['image_embedding'] = train_df['image_embedding'].apply(parse_embedding)\n",
    "image_matrix = np.stack(train_df['image_embedding'].values)\n",
    "print(f\"  âœ“ Shape: {image_matrix.shape}\")\n",
    "print(f\"    Min: {image_matrix.min():.4f}, Max: {image_matrix.max():.4f}\")\n",
    "print(f\"    Mean: {image_matrix.mean():.4f}, Std: {image_matrix.std():.4f}\")\n",
    "\n",
    "# Quality checks\n",
    "print(f\"\\nâœ“ Embedding Quality Checks:\")\n",
    "for name, matrix in [('text', text_matrix), ('title', title_matrix), ('image', image_matrix)]:\n",
    "    has_nan = np.isnan(matrix).any()\n",
    "    has_inf = np.isinf(matrix).any()\n",
    "    variance = matrix.var()\n",
    "    \n",
    "    print(f\"  {name:8s}: NaN={has_nan}, Inf={has_inf}, Variance={variance:.6f}\")\n",
    "    \n",
    "    if has_nan or has_inf:\n",
    "        print(f\"    âŒ WARNING: {name} has invalid values!\")\n",
    "    if variance < 0.001:\n",
    "        print(f\"    âš ï¸ WARNING: {name} has very low variance!\")\n",
    "\n",
    "print(f\"\\nâœ“ Total embedding features: {text_matrix.shape[1] + title_matrix.shape[1] + image_matrix.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a849211-75d0-4ebd-b6f5-ce9ae34e550f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXTRACTING NUMERIC FEATURES\n",
      "================================================================================\n",
      "\n",
      "âœ“ Found 10/10 numeric features:\n",
      "  - value_raw\n",
      "  - pack_count\n",
      "  - total_qty_ml\n",
      "  - unit_qty_ml\n",
      "  - total_qty_g\n",
      "  - unit_qty_g\n",
      "  - total_qty_count\n",
      "  - unit_qty_count\n",
      "  - title_len_tokens\n",
      "  - digits_in_title\n",
      "\n",
      "âœ“ Numeric feature matrix: (75000, 10)\n",
      "\n",
      "  Statistics:\n",
      "    value_raw           : min=0.00, max=63882.00, mean=53.61\n",
      "    pack_count          : min=1.00, max=500.00, mean=3.62\n",
      "    total_qty_ml        : min=0.00, max=63882.00, mean=16.76\n",
      "    unit_qty_ml         : min=0.00, max=63882.00, mean=9.85\n",
      "    total_qty_g         : min=0.00, max=46752.00, mean=38.32\n",
      "    unit_qty_g          : min=0.00, max=46752.00, mean=27.64\n",
      "    total_qty_count     : min=0.00, max=6000.00, mean=9.99\n",
      "    unit_qty_count      : min=0.00, max=6000.00, mean=6.76\n",
      "    title_len_tokens    : min=0.00, max=75.00, mean=14.33\n",
      "    digits_in_title     : min=0.00, max=23.00, mean=3.24\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PHASE 1 - CHUNK 5: Extract ALL Numeric Features\n",
    "You have 10+ numeric features, not just 3!\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTING NUMERIC FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define all numeric features from your columns\n",
    "numeric_feature_names = [\n",
    "    'value_raw',\n",
    "    'pack_count',\n",
    "    'total_qty_ml',\n",
    "    'unit_qty_ml',\n",
    "    'total_qty_g',\n",
    "    'unit_qty_g',\n",
    "    'total_qty_count',\n",
    "    'unit_qty_count',\n",
    "    'title_len_tokens',\n",
    "    'digits_in_title'\n",
    "]\n",
    "\n",
    "# Check which exist\n",
    "existing_numeric = [col for col in numeric_feature_names if col in train_df.columns]\n",
    "missing_numeric = [col for col in numeric_feature_names if col not in train_df.columns]\n",
    "\n",
    "print(f\"\\nâœ“ Found {len(existing_numeric)}/{len(numeric_feature_names)} numeric features:\")\n",
    "for col in existing_numeric:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "if missing_numeric:\n",
    "    print(f\"\\nâš ï¸ Missing numeric features (will skip):\")\n",
    "    for col in missing_numeric:\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "# Fill missing values and extract\n",
    "numeric_data = []\n",
    "for col in existing_numeric:\n",
    "    train_df[col] = train_df[col].fillna(0)\n",
    "    numeric_data.append(train_df[col].values)\n",
    "\n",
    "if numeric_data:\n",
    "    numeric_matrix = np.column_stack(numeric_data)\n",
    "else:\n",
    "    numeric_matrix = np.zeros((len(train_df), 0))\n",
    "\n",
    "print(f\"\\nâœ“ Numeric feature matrix: {numeric_matrix.shape}\")\n",
    "\n",
    "# Statistics\n",
    "if numeric_matrix.shape[1] > 0:\n",
    "    print(f\"\\n  Statistics:\")\n",
    "    for i, col in enumerate(existing_numeric):\n",
    "        print(f\"    {col:20s}: min={numeric_matrix[:, i].min():.2f}, max={numeric_matrix[:, i].max():.2f}, mean={numeric_matrix[:, i].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a4e2c9a-1233-4dfe-8148-8e1128cb6383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXTRACTING BINARY FEATURES\n",
      "================================================================================\n",
      "\n",
      "âœ“ Found 5/5 binary features:\n",
      "  - is_multipack        :  23291 positive ( 31.1%)\n",
      "  - is_organic_like     :  27000 positive ( 36.0%)\n",
      "  - is_bundle           :  10550 positive ( 14.1%)\n",
      "  - multi_flavor        :  16867 positive ( 22.5%)\n",
      "  - bucket4             :      0 positive (  0.0%)\n",
      "\n",
      "âœ“ Binary feature matrix: (75000, 5)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PHASE 1 - CHUNK 6: Extract ALL Binary Features (FIXED)\n",
    "These are CRITICAL for price prediction!\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTING BINARY FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "binary_feature_names = [\n",
    "    'is_multipack',      # Multipacks = bulk discount\n",
    "    'is_organic_like',   # Organic = premium pricing\n",
    "    'is_bundle',         # Bundles = different pricing\n",
    "    'multi_flavor',      # Multiple flavors\n",
    "    'bucket4'            # Category bucket\n",
    "]\n",
    "\n",
    "# Check which exist\n",
    "existing_binary = [col for col in binary_feature_names if col in train_df.columns]\n",
    "missing_binary = [col for col in binary_feature_names if col not in train_df.columns]\n",
    "\n",
    "print(f\"\\nâœ“ Found {len(existing_binary)}/{len(binary_feature_names)} binary features:\")\n",
    "\n",
    "# Extract binary features with proper type conversion\n",
    "binary_data = []\n",
    "for col in existing_binary:\n",
    "    # Convert to numeric first (handles True/False, '1'/'0', etc.)\n",
    "    train_df[col] = pd.to_numeric(train_df[col], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # Now calculate statistics\n",
    "    count_ones = train_df[col].sum()\n",
    "    pct = count_ones / len(train_df) * 100\n",
    "    print(f\"  - {col:20s}: {count_ones:6.0f} positive ({pct:5.1f}%)\")\n",
    "    \n",
    "    binary_data.append(train_df[col].values)\n",
    "\n",
    "if missing_binary:\n",
    "    print(f\"\\nâš ï¸ Missing binary features:\")\n",
    "    for col in missing_binary:\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "# Create binary matrix\n",
    "if binary_data:\n",
    "    binary_matrix = np.column_stack(binary_data)\n",
    "else:\n",
    "    binary_matrix = np.zeros((len(train_df), 0))\n",
    "\n",
    "print(f\"\\nâœ“ Binary feature matrix: {binary_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16478d0d-3348-4098-9259-09c174800ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXTRACTING CONTAINER FEATURES\n",
      "================================================================================\n",
      "\n",
      "âœ“ Found 10/10 container features:\n",
      "  - container_bottle    :   2074 items (  2.8%)\n",
      "  - container_jar       :   1433 items (  1.9%)\n",
      "  - container_can       :   1058 items (  1.4%)\n",
      "  - container_box       :   2503 items (  3.3%)\n",
      "  - container_pouch     :    738 items (  1.0%)\n",
      "  - container_sachet    :     43 items (  0.1%)\n",
      "  - container_pod       :     61 items (  0.1%)\n",
      "  - container_k-cup     :    806 items (  1.1%)\n",
      "  - container_k_cup     :    194 items (  0.3%)\n",
      "  - container_tin       :    377 items (  0.5%)\n",
      "\n",
      "âœ“ Container feature matrix: (75000, 10)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PHASE 1 - CHUNK 7: Extract Container Type Features\n",
    "Container type = HUGE price signal (bottle vs sachet vs jar)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTING CONTAINER FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "container_feature_names = [\n",
    "    'container_bottle',\n",
    "    'container_jar',\n",
    "    'container_can',\n",
    "    'container_box',\n",
    "    'container_pouch',\n",
    "    'container_sachet',\n",
    "    'container_pod',\n",
    "    'container_k-cup',\n",
    "    'container_k_cup',\n",
    "    'container_tin'\n",
    "]\n",
    "\n",
    "# Check which exist\n",
    "existing_containers = [col for col in container_feature_names if col in train_df.columns]\n",
    "missing_containers = [col for col in container_feature_names if col not in train_df.columns]\n",
    "\n",
    "print(f\"\\nâœ“ Found {len(existing_containers)}/{len(container_feature_names)} container features:\")\n",
    "for col in existing_containers:\n",
    "    count_ones = train_df[col].fillna(0).sum()\n",
    "    pct = count_ones / len(train_df) * 100\n",
    "    print(f\"  - {col:20s}: {count_ones:6.0f} items ({pct:5.1f}%)\")\n",
    "\n",
    "if missing_containers:\n",
    "    print(f\"\\nâš ï¸ Missing container features:\")\n",
    "    for col in missing_containers:\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "# Extract container features\n",
    "container_data = []\n",
    "for col in existing_containers:\n",
    "    train_df[col] = train_df[col].fillna(0).astype(int)\n",
    "    container_data.append(train_df[col].values)\n",
    "\n",
    "if container_data:\n",
    "    container_matrix = np.column_stack(container_data)\n",
    "else:\n",
    "    container_matrix = np.zeros((len(train_df), 0))\n",
    "\n",
    "print(f\"\\nâœ“ Container feature matrix: {container_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fe2d7c3-987d-4055-b3d8-6ab2268bb7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TARGET ENCODING CATEGORICAL FEATURES\n",
      "================================================================================\n",
      "\n",
      "âœ“ Found 4/4 categorical features:\n",
      "  - unit_raw       :   57 unique values\n",
      "  - unit_norm      :    6 unique values\n",
      "  - brand          : 28064 unique values\n",
      "  - brand_topk     :  301 unique values\n",
      "\n",
      "  Applying target encoding...\n",
      "  âœ“ Target-encoded shape: (75000, 4)\n",
      "    Range: [1.2736, 4.2875]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PHASE 1 - CHUNK 8: Target Encoding for Categorical Features\n",
    "Brand, unit_raw, etc. have STRONG relationship with price\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TARGET ENCODING CATEGORICAL FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "categorical_feature_names = ['unit_raw', 'unit_norm', 'brand', 'brand_topk']\n",
    "\n",
    "# Check which exist\n",
    "existing_categorical = [col for col in categorical_feature_names if col in train_df.columns]\n",
    "missing_categorical = [col for col in categorical_feature_names if col not in train_df.columns]\n",
    "\n",
    "print(f\"\\nâœ“ Found {len(existing_categorical)}/{len(categorical_feature_names)} categorical features:\")\n",
    "for col in existing_categorical:\n",
    "    unique_count = train_df[col].nunique()\n",
    "    print(f\"  - {col:15s}: {unique_count:4d} unique values\")\n",
    "\n",
    "if missing_categorical:\n",
    "    print(f\"\\nâš ï¸ Missing categorical features:\")\n",
    "    for col in missing_categorical:\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "# Target encoding (uses price information - very powerful!)\n",
    "if existing_categorical:\n",
    "    print(f\"\\n  Applying target encoding...\")\n",
    "    \n",
    "    # Log-transform target for encoding\n",
    "    train_df['price_log'] = np.log1p(train_df['price'])\n",
    "    \n",
    "    # Target encoder\n",
    "    target_encoder = TargetEncoder(cols=existing_categorical, smoothing=10)\n",
    "    categorical_encoded = target_encoder.fit_transform(\n",
    "        train_df[existing_categorical],\n",
    "        train_df['price_log']\n",
    "    )\n",
    "    \n",
    "    categorical_matrix = categorical_encoded.values\n",
    "    \n",
    "    print(f\"  âœ“ Target-encoded shape: {categorical_matrix.shape}\")\n",
    "    print(f\"    Range: [{categorical_matrix.min():.4f}, {categorical_matrix.max():.4f}]\")\n",
    "else:\n",
    "    categorical_matrix = np.zeros((len(train_df), 0))\n",
    "    target_encoder = None\n",
    "    print(f\"\\n  âš ï¸ No categorical features to encode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9f40715-6191-4351-a6d8-7927e42dd4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMBINING ALL FEATURES\n",
      "================================================================================\n",
      "\n",
      "âœ“ COMPLETE FEATURE MATRIX: (75000, 2333)\n",
      "\n",
      "  Feature breakdown:\n",
      "    text_embeddings          :  1024 features\n",
      "    title_embeddings         :   512 features\n",
      "    image_embeddings         :   768 features\n",
      "    numeric_features         :    10 features\n",
      "    binary_features          :     5 features\n",
      "    container_features       :    10 features\n",
      "    categorical_features     :     4 features\n",
      "\n",
      "  Total features: 2333\n",
      "  Samples: 75000\n",
      "\n",
      "  Quality checks:\n",
      "    Has NaN: False\n",
      "    Has Inf: False\n",
      "    Range: [-14.8205, 63882.0000]\n",
      "\n",
      "  Target (log-price):\n",
      "    Range: [0.1222, 7.9363]\n",
      "    Mean: 2.7392\n",
      "\n",
      "================================================================================\n",
      "SAVING FEATURES & METADATA\n",
      "================================================================================\n",
      "\n",
      "âœ“ Saved feature files:\n",
      "  - X_full_features.npy ((75000, 2333))\n",
      "  - y_log_target.npy ((75000,))\n",
      "  - y_original_prices.npy ((75000,))\n",
      "âœ“ Saved metadata:\n",
      "  - feature_metadata.pkl\n",
      "  - target_encoder.pkl\n",
      "\n",
      "================================================================================\n",
      "âœ… PHASE 1 COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Summary:\n",
      "  You were using: 3 features before\n",
      "  Now using: 2333 features\n",
      "  That's 777x MORE information!\n",
      "\n",
      "ðŸš€ Next: Run Phase 1.5 (Hyperparameter Tuning)\n",
      "  Expected time with A100: 60-80 minutes\n",
      "  Expected SMAPE improvement: -5 to -7%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PHASE 1 - CHUNK 9: Combine ALL Features into Final Matrix\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMBINING ALL FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine everything\n",
    "feature_matrices = []\n",
    "feature_names = []\n",
    "feature_counts = {}\n",
    "\n",
    "# Embeddings\n",
    "if text_matrix.shape[1] > 0:\n",
    "    feature_matrices.append(text_matrix)\n",
    "    feature_names.append('text_embeddings')\n",
    "    feature_counts['text_embeddings'] = text_matrix.shape[1]\n",
    "\n",
    "if title_matrix.shape[1] > 0:\n",
    "    feature_matrices.append(title_matrix)\n",
    "    feature_names.append('title_embeddings')\n",
    "    feature_counts['title_embeddings'] = title_matrix.shape[1]\n",
    "\n",
    "if image_matrix.shape[1] > 0:\n",
    "    feature_matrices.append(image_matrix)\n",
    "    feature_names.append('image_embeddings')\n",
    "    feature_counts['image_embeddings'] = image_matrix.shape[1]\n",
    "\n",
    "# Engineered features\n",
    "if numeric_matrix.shape[1] > 0:\n",
    "    feature_matrices.append(numeric_matrix)\n",
    "    feature_names.append('numeric_features')\n",
    "    feature_counts['numeric_features'] = numeric_matrix.shape[1]\n",
    "\n",
    "if binary_matrix.shape[1] > 0:\n",
    "    feature_matrices.append(binary_matrix)\n",
    "    feature_names.append('binary_features')\n",
    "    feature_counts['binary_features'] = binary_matrix.shape[1]\n",
    "\n",
    "if container_matrix.shape[1] > 0:\n",
    "    feature_matrices.append(container_matrix)\n",
    "    feature_names.append('container_features')\n",
    "    feature_counts['container_features'] = container_matrix.shape[1]\n",
    "\n",
    "if categorical_matrix.shape[1] > 0:\n",
    "    feature_matrices.append(categorical_matrix)\n",
    "    feature_names.append('categorical_features')\n",
    "    feature_counts['categorical_features'] = categorical_matrix.shape[1]\n",
    "\n",
    "# Stack all features\n",
    "X_full = np.hstack(feature_matrices)\n",
    "y = np.log1p(train_df['price'].values)\n",
    "y_original = train_df['price'].values\n",
    "\n",
    "print(f\"\\nâœ“ COMPLETE FEATURE MATRIX: {X_full.shape}\")\n",
    "print(f\"\\n  Feature breakdown:\")\n",
    "for name in feature_names:\n",
    "    print(f\"    {name:25s}: {feature_counts[name]:5d} features\")\n",
    "\n",
    "print(f\"\\n  Total features: {X_full.shape[1]}\")\n",
    "print(f\"  Samples: {X_full.shape[0]}\")\n",
    "\n",
    "print(f\"\\n  Quality checks:\")\n",
    "print(f\"    Has NaN: {np.isnan(X_full).any()}\")\n",
    "print(f\"    Has Inf: {np.isinf(X_full).any()}\")\n",
    "print(f\"    Range: [{X_full.min():.4f}, {X_full.max():.4f}]\")\n",
    "\n",
    "print(f\"\\n  Target (log-price):\")\n",
    "print(f\"    Range: [{y.min():.4f}, {y.max():.4f}]\")\n",
    "print(f\"    Mean: {y.mean():.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE EVERYTHING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING FEATURES & METADATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save feature matrices\n",
    "np.save('X_full_features.npy', X_full)\n",
    "np.save('y_log_target.npy', y)\n",
    "np.save('y_original_prices.npy', y_original)\n",
    "\n",
    "print(f\"\\nâœ“ Saved feature files:\")\n",
    "print(f\"  - X_full_features.npy ({X_full.shape})\")\n",
    "print(f\"  - y_log_target.npy ({y.shape})\")\n",
    "print(f\"  - y_original_prices.npy ({y_original.shape})\")\n",
    "\n",
    "# Save encoders and metadata\n",
    "metadata = {\n",
    "    'feature_counts': feature_counts,\n",
    "    'feature_names': feature_names,\n",
    "    'numeric_features': existing_numeric,\n",
    "    'binary_features': existing_binary,\n",
    "    'container_features': existing_containers,\n",
    "    'categorical_features': existing_categorical,\n",
    "    'text_dim': text_matrix.shape[1],\n",
    "    'title_dim': title_matrix.shape[1],\n",
    "    'image_dim': image_matrix.shape[1],\n",
    "    'total_features': X_full.shape[1],\n",
    "    'n_samples': X_full.shape[0]\n",
    "}\n",
    "\n",
    "with open('feature_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "with open('target_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(target_encoder, f)\n",
    "\n",
    "print(f\"âœ“ Saved metadata:\")\n",
    "print(f\"  - feature_metadata.pkl\")\n",
    "print(f\"  - target_encoder.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… PHASE 1 COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“Š Summary:\")\n",
    "print(f\"  You were using: 3 features before\")\n",
    "print(f\"  Now using: {X_full.shape[1]} features\")\n",
    "print(f\"  That's {X_full.shape[1]//3}x MORE information!\")\n",
    "\n",
    "print(f\"\\nðŸš€ Next: Run Phase 1.5 (Hyperparameter Tuning)\")\n",
    "print(f\"  Expected time with A100: 60-80 minutes\")\n",
    "print(f\"  Expected SMAPE improvement: -5 to -7%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e85ec71-66ac-4e4c-8ad5-d336e2e54391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RECOVERY MODE - RESTARTING TUNING\n",
      "================================================================================\n",
      "\n",
      "[1/2] Checking saved files...\n",
      "  âœ“ X_full_features.npy\n",
      "  âœ“ y_log_target.npy\n",
      "  âœ“ y_original_prices.npy\n",
      "  âœ“ feature_metadata.pkl\n",
      "\n",
      "[2/2] Loading features...\n",
      "  âœ“ Features: (75000, 2333)\n",
      "  âœ“ Targets: (75000,)\n",
      "  âœ“ Metadata: 11 keys\n",
      "\n",
      "âœ… Recovery successful! Ready to tune.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RECOVERY: Restart Hyperparameter Tuning\n",
    "Run this after kernel disconnect\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "# Optuna\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RECOVERY MODE - RESTARTING TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFY FILES EXIST\n",
    "# ============================================================================\n",
    "print(\"\\n[1/2] Checking saved files...\")\n",
    "\n",
    "required_files = [\n",
    "    'X_full_features.npy',\n",
    "    'y_log_target.npy', \n",
    "    'y_original_prices.npy',\n",
    "    'feature_metadata.pkl'\n",
    "]\n",
    "\n",
    "missing_files = []\n",
    "for file in required_files:\n",
    "    try:\n",
    "        if file.endswith('.npy'):\n",
    "            np.load(file)\n",
    "        else:\n",
    "            with open(file, 'rb') as f:\n",
    "                pickle.load(f)\n",
    "        print(f\"  âœ“ {file}\")\n",
    "    except FileNotFoundError:\n",
    "        missing_files.append(file)\n",
    "        print(f\"  âŒ {file} - MISSING!\")\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\nâŒ Missing files: {missing_files}\")\n",
    "    print(\"   You need to re-run Phase 1!\")\n",
    "    raise FileNotFoundError(\"Required files missing\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "print(\"\\n[2/2] Loading features...\")\n",
    "\n",
    "X_full = np.load('X_full_features.npy')\n",
    "y = np.load('y_log_target.npy')\n",
    "y_original = np.load('y_original_prices.npy')\n",
    "\n",
    "with open('feature_metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "print(f\"  âœ“ Features: {X_full.shape}\")\n",
    "print(f\"  âœ“ Targets: {y.shape}\")\n",
    "print(f\"  âœ“ Metadata: {len(metadata)} keys\")\n",
    "\n",
    "print(\"\\nâœ… Recovery successful! Ready to tune.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e742edaa-9095-420a-b737-4ab25b8e30f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b21d7556-50bb-462b-a3f3-033632661d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUICK LIGHTGBM TRAINING (NO TUNING)\n",
      "================================================================================\n",
      "âœ“ Data loaded: (75000, 2333)\n",
      "\n",
      "[1/2] Using optimized default parameters...\n",
      "âœ“ Parameters set (proven defaults)\n",
      "\n",
      "[2/2] Training LightGBM...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[200]\tvalid_0's l1: 0.372493\n",
      "[400]\tvalid_0's l1: 0.367576\n",
      "[600]\tvalid_0's l1: 0.365368\n",
      "[800]\tvalid_0's l1: 0.364166\n",
      "[1000]\tvalid_0's l1: 0.363378\n",
      "[1200]\tvalid_0's l1: 0.363073\n",
      "Early stopping, best iteration is:\n",
      "[1155]\tvalid_0's l1: 0.363031\n",
      "\n",
      "âœ“ Training complete!\n",
      "  Best iteration: 1155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation SMAPE: 37.7346%\n",
      "\n",
      "[3/3] Saving model...\n",
      "\n",
      "âœ… SAVED:\n",
      "  - lgb_model_final.pkl\n",
      "  - best_lgb_params.pkl\n",
      "  - lgb_metadata.pkl\n",
      "\n",
      "================================================================================\n",
      "âœ… LIGHTGBM DONE! (2-3 minutes)\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š LightGBM SMAPE: 37.7346%\n",
      "\n",
      "ðŸ”„ Now restart kernel and run GPU tuning for CatBoost/XGBoost\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "QUICK STEP 1: Train LightGBM with Good Defaults (NO TUNING)\n",
    "Expected time: 2-3 minutes\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"QUICK LIGHTGBM TRAINING (NO TUNING)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load features\n",
    "X_full = np.load('X_full_features.npy')\n",
    "y = np.load('y_log_target.npy')\n",
    "y_original = np.load('y_original_prices.npy')\n",
    "\n",
    "print(f\"âœ“ Data loaded: {X_full.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PROVEN GOOD DEFAULT PARAMETERS\n",
    "# ============================================================================\n",
    "print(\"\\n[1/2] Using optimized default parameters...\")\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'huber',\n",
    "    'metric': 'mae',\n",
    "    'num_leaves': 127,\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.03,\n",
    "    'n_estimators': 2000,\n",
    "    'min_child_samples': 20,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.5,\n",
    "    'reg_lambda': 1.0,\n",
    "    'min_gain_to_split': 0.1,\n",
    "    'verbosity': -1,\n",
    "    'n_jobs': -1,\n",
    "    'device': 'cpu',\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "print(f\"âœ“ Parameters set (proven defaults)\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAIN MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n[2/2] Training LightGBM...\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "idx_train, idx_val = train_test_split(\n",
    "    range(len(y)), test_size=0.2, random_state=42\n",
    ")\n",
    "y_val_orig = y_original[idx_val]\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "lgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(200)]\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Training complete!\")\n",
    "print(f\"  Best iteration: {lgb_model.best_iteration_}\")\n",
    "\n",
    "# Evaluate\n",
    "y_pred = np.expm1(lgb_model.predict(X_val))\n",
    "lgb_smape = np.mean(2 * np.abs(y_pred - y_val_orig) / (np.abs(y_pred) + np.abs(y_val_orig))) * 100\n",
    "\n",
    "print(f\"  Validation SMAPE: {lgb_smape:.4f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE EVERYTHING\n",
    "# ============================================================================\n",
    "print(\"\\n[3/3] Saving model...\")\n",
    "\n",
    "with open('lgb_model_final.pkl', 'wb') as f:\n",
    "    pickle.dump(lgb_model, f)\n",
    "\n",
    "with open('best_lgb_params.pkl', 'wb') as f:\n",
    "    pickle.dump(lgb_params, f)\n",
    "\n",
    "lgb_metadata = {\n",
    "    'best_smape': lgb_smape,\n",
    "    'best_params': lgb_params,\n",
    "    'best_iteration': lgb_model.best_iteration_,\n",
    "    'n_features': X_full.shape[1]\n",
    "}\n",
    "\n",
    "with open('lgb_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(lgb_metadata, f)\n",
    "\n",
    "print(f\"\\nâœ… SAVED:\")\n",
    "print(f\"  - lgb_model_final.pkl\")\n",
    "print(f\"  - best_lgb_params.pkl\")\n",
    "print(f\"  - lgb_metadata.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… LIGHTGBM DONE! (2-3 minutes)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ“Š LightGBM SMAPE: {lgb_smape:.4f}%\")\n",
    "print(f\"\\nðŸ”„ Now restart kernel and run GPU tuning for CatBoost/XGBoost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d567cf-a044-4a9e-9433-f4d2bbfcb315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING LIBRARIES AND DATA AFTER RESTART\n",
      "================================================================================\n",
      "\n",
      "âœ“ Libraries imported\n",
      "\n",
      "âœ“ GPU Detected: NVIDIA L40S\n",
      "  Memory: 47.9 GB\n",
      "\n",
      "================================================================================\n",
      "LOADING SAVED DATA FILES\n",
      "================================================================================\n",
      "\n",
      "[1/8] Checking files...\n",
      "  âœ“ X_full_features.npy - shape: (75000, 2333)\n",
      "  âœ“ y_log_target.npy - shape: (75000,)\n",
      "  âœ“ y_original_prices.npy - shape: (75000,)\n",
      "  âœ“ lgb_model_final.pkl\n",
      "  âœ“ lgb_metadata.pkl\n",
      "  âœ“ best_lgb_params.pkl\n",
      "  âœ“ feature_metadata.pkl\n",
      "  âœ“ target_encoder.pkl\n",
      "\n",
      "[2/8] Loading feature matrix...\n",
      "  âœ“ Shape: (75000, 2333)\n",
      "    Features: 2333\n",
      "    Samples: 75000\n",
      "\n",
      "[3/8] Loading targets...\n",
      "  âœ“ Log-target shape: (75000,)\n",
      "    Range: [0.1222, 7.9363]\n",
      "  âœ“ Original prices shape: (75000,)\n",
      "    Range: [$0.13, $2796.00]\n",
      "\n",
      "[4/8] Loading LightGBM model...\n",
      "  âœ“ LightGBM model loaded\n",
      "\n",
      "[5/8] Loading LightGBM metadata...\n",
      "  âœ“ LightGBM SMAPE: 37.7346%\n",
      "    Best iteration: 1155\n",
      "    Features used: 2333\n",
      "\n",
      "[6/8] Loading LightGBM parameters...\n",
      "  âœ“ Parameters loaded\n",
      "    Learning rate: 0.03\n",
      "    Num leaves: 127\n",
      "    Max depth: 10\n",
      "\n",
      "[7/8] Loading feature metadata...\n",
      "  âœ“ Metadata loaded\n",
      "    Feature groups:\n",
      "      - text_embeddings          : 1024 features\n",
      "      - title_embeddings         :  512 features\n",
      "      - image_embeddings         :  768 features\n",
      "      - numeric_features         :   10 features\n",
      "      - binary_features          :    5 features\n",
      "      - container_features       :   10 features\n",
      "      - categorical_features     :    4 features\n",
      "\n",
      "[8/8] Loading target encoder...\n",
      "  âœ“ Target encoder loaded\n",
      "    Encoded features: ['unit_raw', 'unit_norm', 'brand', 'brand_topk']\n",
      "\n",
      "================================================================================\n",
      "âœ… ALL DATA LOADED SUCCESSFULLY!\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Available Data:\n",
      "  Features (X_full):        (75000, 2333)\n",
      "  Log targets (y):          (75000,)\n",
      "  Original prices:          (75000,)\n",
      "  LightGBM model:           Loaded (SMAPE: 37.7346%)\n",
      "\n",
      "ðŸŽ¯ Feature Breakdown:\n",
      "  Text embeddings:          1024\n",
      "  Title embeddings:         512\n",
      "  Image embeddings:         768\n",
      "  Numeric features:         10\n",
      "  Binary features:          5\n",
      "  Container features:       10\n",
      "  Categorical features:     4\n",
      "  Total:                    2333\n",
      "\n",
      "ðŸš€ Ready to run STEP 2 (CatBoost + XGBoost tuning)!\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "âœ“ Testing LightGBM model...\n",
      "  Sample predictions (log): [1.7881261  2.64002104 1.21835238 3.30883439 4.06531518]\n",
      "  Sample predictions ($): [ 4.97823936 13.01349841  2.38161154 26.35322363 57.28327525]\n",
      "\n",
      "âœ“ All systems ready!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AFTER KERNEL RESTART: Load Everything\n",
    "Run this FIRST after restarting kernel\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING LIBRARIES AND DATA AFTER RESTART\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Hyperparameter tuning\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "print(\"\\nâœ“ Libraries imported\")\n",
    "\n",
    "# ============================================================================\n",
    "# GPU CONFIGURATION\n",
    "# ============================================================================\n",
    "import torch\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nâœ“ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ CPU mode\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD ALL SAVED FILES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING SAVED DATA FILES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Required files\n",
    "required_files = {\n",
    "    'features': 'X_full_features.npy',\n",
    "    'target_log': 'y_log_target.npy',\n",
    "    'target_original': 'y_original_prices.npy',\n",
    "    'lgb_model': 'lgb_model_final.pkl',\n",
    "    'lgb_metadata': 'lgb_metadata.pkl',\n",
    "    'lgb_params': 'best_lgb_params.pkl',\n",
    "    'metadata': 'feature_metadata.pkl',\n",
    "    'target_encoder': 'target_encoder.pkl'\n",
    "}\n",
    "\n",
    "print(\"\\n[1/8] Checking files...\")\n",
    "missing_files = []\n",
    "for name, filepath in required_files.items():\n",
    "    try:\n",
    "        if filepath.endswith('.npy'):\n",
    "            data = np.load(filepath)\n",
    "            print(f\"  âœ“ {filepath} - shape: {data.shape}\")\n",
    "        else:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            print(f\"  âœ“ {filepath}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  âŒ {filepath} - MISSING!\")\n",
    "        missing_files.append(filepath)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\nâŒ ERROR: Missing files: {missing_files}\")\n",
    "    print(\"   You need to run Phase 1 and STEP 1 first!\")\n",
    "    raise FileNotFoundError(f\"Missing required files: {missing_files}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n[2/8] Loading feature matrix...\")\n",
    "X_full = np.load('X_full_features.npy')\n",
    "print(f\"  âœ“ Shape: {X_full.shape}\")\n",
    "print(f\"    Features: {X_full.shape[1]}\")\n",
    "print(f\"    Samples: {X_full.shape[0]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD TARGETS\n",
    "# ============================================================================\n",
    "print(\"\\n[3/8] Loading targets...\")\n",
    "y = np.load('y_log_target.npy')\n",
    "y_original = np.load('y_original_prices.npy')\n",
    "print(f\"  âœ“ Log-target shape: {y.shape}\")\n",
    "print(f\"    Range: [{y.min():.4f}, {y.max():.4f}]\")\n",
    "print(f\"  âœ“ Original prices shape: {y_original.shape}\")\n",
    "print(f\"    Range: [${y_original.min():.2f}, ${y_original.max():.2f}]\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD LIGHTGBM MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n[4/8] Loading LightGBM model...\")\n",
    "with open('lgb_model_final.pkl', 'rb') as f:\n",
    "    lgb_model = pickle.load(f)\n",
    "print(f\"  âœ“ LightGBM model loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD LIGHTGBM METADATA\n",
    "# ============================================================================\n",
    "print(\"\\n[5/8] Loading LightGBM metadata...\")\n",
    "with open('lgb_metadata.pkl', 'rb') as f:\n",
    "    lgb_metadata = pickle.load(f)\n",
    "print(f\"  âœ“ LightGBM SMAPE: {lgb_metadata['best_smape']:.4f}%\")\n",
    "print(f\"    Best iteration: {lgb_metadata['best_iteration']}\")\n",
    "print(f\"    Features used: {lgb_metadata['n_features']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD LIGHTGBM PARAMS\n",
    "# ============================================================================\n",
    "print(\"\\n[6/8] Loading LightGBM parameters...\")\n",
    "with open('best_lgb_params.pkl', 'rb') as f:\n",
    "    lgb_params = pickle.load(f)\n",
    "print(f\"  âœ“ Parameters loaded\")\n",
    "print(f\"    Learning rate: {lgb_params.get('learning_rate', 'N/A')}\")\n",
    "print(f\"    Num leaves: {lgb_params.get('num_leaves', 'N/A')}\")\n",
    "print(f\"    Max depth: {lgb_params.get('max_depth', 'N/A')}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD FEATURE METADATA\n",
    "# ============================================================================\n",
    "print(\"\\n[7/8] Loading feature metadata...\")\n",
    "with open('feature_metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "print(f\"  âœ“ Metadata loaded\")\n",
    "print(f\"    Feature groups:\")\n",
    "for name, count in metadata['feature_counts'].items():\n",
    "    print(f\"      - {name:25s}: {count:4d} features\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD TARGET ENCODER\n",
    "# ============================================================================\n",
    "print(\"\\n[8/8] Loading target encoder...\")\n",
    "with open('target_encoder.pkl', 'rb') as f:\n",
    "    target_encoder = pickle.load(f)\n",
    "print(f\"  âœ“ Target encoder loaded\")\n",
    "if target_encoder:\n",
    "    print(f\"    Encoded features: {metadata['categorical_features']}\")\n",
    "else:\n",
    "    print(f\"    No categorical encoding used\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… ALL DATA LOADED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“Š Available Data:\")\n",
    "print(f\"  Features (X_full):        {X_full.shape}\")\n",
    "print(f\"  Log targets (y):          {y.shape}\")\n",
    "print(f\"  Original prices:          {y_original.shape}\")\n",
    "print(f\"  LightGBM model:           Loaded (SMAPE: {lgb_metadata['best_smape']:.4f}%)\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Feature Breakdown:\")\n",
    "print(f\"  Text embeddings:          {metadata['text_dim']}\")\n",
    "print(f\"  Title embeddings:         {metadata['title_dim']}\")\n",
    "print(f\"  Image embeddings:         {metadata['image_dim']}\")\n",
    "print(f\"  Numeric features:         {len(metadata['numeric_features'])}\")\n",
    "print(f\"  Binary features:          {len(metadata['binary_features'])}\")\n",
    "print(f\"  Container features:       {len(metadata['container_features'])}\")\n",
    "print(f\"  Categorical features:     {len(metadata['categorical_features'])}\")\n",
    "print(f\"  Total:                    {X_full.shape[1]}\")\n",
    "\n",
    "print(f\"\\nðŸš€ Ready to run STEP 2 (CatBoost + XGBoost tuning)!\")\n",
    "\n",
    "# ============================================================================\n",
    "# QUICK VERIFICATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test LightGBM prediction\n",
    "print(\"\\nâœ“ Testing LightGBM model...\")\n",
    "X_test_sample = X_full[:5]\n",
    "lgb_test_pred = lgb_model.predict(X_test_sample)\n",
    "print(f\"  Sample predictions (log): {lgb_test_pred}\")\n",
    "print(f\"  Sample predictions ($): {np.expm1(lgb_test_pred)}\")\n",
    "\n",
    "print(\"\\nâœ“ All systems ready!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca902191-bc5d-4e10-bab2-b3766a8b6dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c7d3357-0d8d-4901-9a9b-40f35cb95a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 2 (FAST): TRAIN GPU MODELS - NO TUNING\n",
      "================================================================================\n",
      "\n",
      "[1/3] Preparing data...\n",
      "âœ“ Train: 60000, Val: 15000\n",
      "\n",
      "================================================================================\n",
      "[2/3] TRAINING CATBOOST (GPU - DEFAULT PARAMS)\n",
      "================================================================================\n",
      "\n",
      "ðŸ“‹ Parameters:\n",
      "  loss_function            : RMSE\n",
      "  iterations               : 2000\n",
      "  learning_rate            : 0.03\n",
      "  depth                    : 8\n",
      "  l2_leaf_reg              : 3.0\n",
      "  bagging_temperature      : 0.5\n",
      "  random_strength          : 1.0\n",
      "  border_count             : 128\n",
      "\n",
      "â³ Training CatBoost...\n",
      "0:\tlearn: 0.9236227\ttest: 0.9361379\tbest: 0.9361379 (0)\ttotal: 105ms\tremaining: 3m 30s\n",
      "200:\tlearn: 0.5350226\ttest: 0.5538395\tbest: 0.5538395 (200)\ttotal: 6.13s\tremaining: 54.9s\n",
      "400:\tlearn: 0.5072214\ttest: 0.5443634\tbest: 0.5443634 (400)\ttotal: 12.9s\tremaining: 51.5s\n",
      "600:\tlearn: 0.4838929\ttest: 0.5395708\tbest: 0.5395708 (600)\ttotal: 19.6s\tremaining: 45.7s\n",
      "800:\tlearn: 0.4640882\ttest: 0.5369849\tbest: 0.5369849 (800)\ttotal: 26.4s\tremaining: 39.5s\n",
      "1000:\tlearn: 0.4460048\ttest: 0.5347812\tbest: 0.5347812 (1000)\ttotal: 33.1s\tremaining: 33s\n",
      "1200:\tlearn: 0.4293896\ttest: 0.5331170\tbest: 0.5331170 (1200)\ttotal: 39.7s\tremaining: 26.4s\n",
      "1400:\tlearn: 0.4146108\ttest: 0.5318646\tbest: 0.5318646 (1400)\ttotal: 46.3s\tremaining: 19.8s\n",
      "1600:\tlearn: 0.4002452\ttest: 0.5306870\tbest: 0.5306870 (1600)\ttotal: 53s\tremaining: 13.2s\n",
      "1800:\tlearn: 0.3865693\ttest: 0.5296987\tbest: 0.5296935 (1798)\ttotal: 59.6s\tremaining: 6.59s\n",
      "1999:\tlearn: 0.3739249\ttest: 0.5288545\tbest: 0.5288485 (1998)\ttotal: 1m 6s\tremaining: 0us\n",
      "bestTest = 0.5288485244\n",
      "bestIteration = 1998\n",
      "Shrink model to first 1999 iterations.\n",
      "\n",
      "âœ… CatBoost Training Complete!\n",
      "   Time: 1.8 minutes\n",
      "   Validation SMAPE: 39.0249%\n",
      "   Best iteration: 1998\n",
      "âœ“ Saved: cat_model_final.pkl\n",
      "\n",
      "================================================================================\n",
      "[3/3] TRAINING XGBOOST (GPU - DEFAULT PARAMS)\n",
      "================================================================================\n",
      "\n",
      "ðŸ“‹ Parameters:\n",
      "  objective                : reg:pseudohubererror\n",
      "  max_depth                : 8\n",
      "  learning_rate            : 0.03\n",
      "  n_estimators             : 2000\n",
      "  min_child_weight         : 3\n",
      "  subsample                : 0.8\n",
      "  colsample_bytree         : 0.8\n",
      "  colsample_bylevel        : 0.8\n",
      "  reg_alpha                : 0.5\n",
      "  reg_lambda               : 1.0\n",
      "  gamma                    : 0.1\n",
      "  max_bin                  : 256\n",
      "\n",
      "â³ Training XGBoost...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 130\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâ³ Training XGBoost...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mxgb_params)\n\u001b[0;32m--> 130\u001b[0m \u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[1;32m    135\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m xgb_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m xgb_start\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 2 (FAST): Train CatBoost + XGBoost with Good Defaults\n",
    "NO HYPERPARAMETER TUNING\n",
    "Expected time: 5-6 minutes\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 2 (FAST): TRAIN GPU MODELS - NO TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Split data\n",
    "print(\"\\n[1/3] Preparing data...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full, y, test_size=0.2, random_state=42\n",
    ")\n",
    "idx_train, idx_val = train_test_split(\n",
    "    range(len(y)), test_size=0.2, random_state=42\n",
    ")\n",
    "y_val_orig = y_original[idx_val]\n",
    "\n",
    "print(f\"âœ“ Train: {len(X_train)}, Val: {len(X_val)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CATBOOST - PROVEN GOOD DEFAULTS (GPU)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2/3] TRAINING CATBOOST (GPU - DEFAULT PARAMS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cat_start = time.time()\n",
    "\n",
    "cat_params = {\n",
    "    'loss_function': 'RMSE',\n",
    "    'iterations': 2000,\n",
    "    'learning_rate': 0.03,\n",
    "    'depth': 8,\n",
    "    'l2_leaf_reg': 3.0,\n",
    "    'bagging_temperature': 0.5,\n",
    "    'random_strength': 1.0,\n",
    "    'border_count': 128,\n",
    "    'verbose': 200,\n",
    "    'random_seed': 42,\n",
    "    'task_type': 'GPU',  # ðŸ”¥ GPU\n",
    "    'devices': '0',\n",
    "    'gpu_ram_part': 0.9\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ“‹ Parameters:\")\n",
    "for key, value in cat_params.items():\n",
    "    if key not in ['verbose', 'task_type', 'devices', 'gpu_ram_part', 'random_seed']:\n",
    "        print(f\"  {key:25s}: {value}\")\n",
    "\n",
    "print(\"\\nâ³ Training CatBoost...\")\n",
    "\n",
    "cat_model = cb.CatBoostRegressor(**cat_params)\n",
    "cat_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=(X_val, y_val),\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "cat_time = time.time() - cat_start\n",
    "\n",
    "# Evaluate\n",
    "cat_pred = np.expm1(cat_model.predict(X_val))\n",
    "cat_smape = np.mean(2 * np.abs(cat_pred - y_val_orig) / (np.abs(cat_pred) + np.abs(y_val_orig))) * 100\n",
    "\n",
    "print(f\"\\nâœ… CatBoost Training Complete!\")\n",
    "print(f\"   Time: {cat_time/60:.1f} minutes\")\n",
    "print(f\"   Validation SMAPE: {cat_smape:.4f}%\")\n",
    "print(f\"   Best iteration: {cat_model.get_best_iteration()}\")\n",
    "\n",
    "# Save\n",
    "with open('cat_model_final.pkl', 'wb') as f:\n",
    "    pickle.dump(cat_model, f)\n",
    "with open('best_cat_params.pkl', 'wb') as f:\n",
    "    pickle.dump(cat_params, f)\n",
    "\n",
    "print(f\"âœ“ Saved: cat_model_final.pkl\")\n",
    "\n",
    "# ============================================================================\n",
    "# XGBOOST - PROVEN GOOD DEFAULTS (GPU)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[3/3] TRAINING XGBOOST (GPU - DEFAULT PARAMS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "xgb_start = time.time()\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'reg:pseudohubererror',\n",
    "    'tree_method': 'gpu_hist',  # ðŸ”¥ GPU\n",
    "    'predictor': 'gpu_predictor',\n",
    "    'gpu_id': 0,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.03,\n",
    "    'n_estimators': 2000,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'colsample_bylevel': 0.8,\n",
    "    'reg_alpha': 0.5,\n",
    "    'reg_lambda': 1.0,\n",
    "    'gamma': 0.1,\n",
    "    'max_bin': 256,\n",
    "    'verbosity': 1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ“‹ Parameters:\")\n",
    "for key, value in xgb_params.items():\n",
    "    if key not in ['tree_method', 'predictor', 'gpu_id', 'verbosity', 'random_state']:\n",
    "        print(f\"  {key:25s}: {value}\")\n",
    "\n",
    "print(\"\\nâ³ Training XGBoost...\")\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "xgb_time = time.time() - xgb_start\n",
    "\n",
    "# Evaluate\n",
    "xgb_pred = np.expm1(xgb_model.predict(X_val))\n",
    "xgb_smape = np.mean(2 * np.abs(xgb_pred - y_val_orig) / (np.abs(xgb_pred) + np.abs(y_val_orig))) * 100\n",
    "\n",
    "print(f\"\\nâœ… XGBoost Training Complete!\")\n",
    "print(f\"   Time: {xgb_time/60:.1f} minutes\")\n",
    "print(f\"   Validation SMAPE: {xgb_smape:.4f}%\")\n",
    "print(f\"   Best iteration: {xgb_model.best_iteration}\")\n",
    "\n",
    "# Save\n",
    "with open('xgb_model_final.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "with open('best_xgb_params.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_params, f)\n",
    "\n",
    "print(f\"âœ“ Saved: xgb_model_final.pkl\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… ALL 3 MODELS TRAINED!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nâ±ï¸ Training Times:\")\n",
    "print(f\"   CatBoost:   {cat_time/60:.1f} minutes\")\n",
    "print(f\"   XGBoost:    {xgb_time/60:.1f} minutes\")\n",
    "print(f\"   Total:      {total_time/60:.1f} minutes\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Individual Model Performance (Validation):\")\n",
    "print(f\"   LightGBM:  {lgb_metadata['best_smape']:.4f}% (default params)\")\n",
    "print(f\"   CatBoost:  {cat_smape:.4f}% (default params)\")\n",
    "print(f\"   XGBoost:   {xgb_smape:.4f}% (default params)\")\n",
    "\n",
    "# Calculate expected ensemble\n",
    "individual_scores = [lgb_metadata['best_smape'], cat_smape, xgb_smape]\n",
    "best_individual = min(individual_scores)\n",
    "avg_score = np.mean(individual_scores)\n",
    "estimated_ensemble = best_individual - 1.5  # Ensemble typically 1-2% better\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Ensemble Estimates:\")\n",
    "print(f\"   Best individual:    {best_individual:.4f}%\")\n",
    "print(f\"   Average:            {avg_score:.4f}%\")\n",
    "print(f\"   Expected ensemble:  ~{estimated_ensemble:.2f}%\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ Saved Files:\")\n",
    "print(f\"   âœ“ lgb_model_final.pkl\")\n",
    "print(f\"   âœ“ cat_model_final.pkl\")\n",
    "print(f\"   âœ“ xgb_model_final.pkl\")\n",
    "print(f\"   âœ“ best_lgb_params.pkl\")\n",
    "print(f\"   âœ“ best_cat_params.pkl\")\n",
    "print(f\"   âœ“ best_xgb_params.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸš€ READY FOR PHASE 2: ENSEMBLE & INFERENCE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“ What's Next:\")\n",
    "print(\"   1. Create weighted ensemble of 3 models\")\n",
    "print(\"   2. Generate predictions on validation set\")\n",
    "print(\"   3. Generate predictions on test set\")\n",
    "print(\"   4. Create submission file\")\n",
    "\n",
    "print(\"\\nâœ¨ Tell me when ready for Phase 2 (Ensemble & Inference)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfc9d417-c353-4ee6-be0a-8e73021d0151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "UNIVERSAL XGBOOST TRAINING\n",
      "================================================================================\n",
      "XGBoost version: 3.0.5\n",
      "âœ“ Train: 60000, Val: 15000\n",
      "\n",
      "ðŸ“‹ Training XGBoost with native API...\n",
      "[0]\ttrain-rmse:0.91251\tvalidation-rmse:0.92516\n",
      "[50]\ttrain-rmse:0.53619\tvalidation-rmse:0.56625\n",
      "[100]\ttrain-rmse:0.49361\tvalidation-rmse:0.54539\n",
      "[150]\ttrain-rmse:0.46871\tvalidation-rmse:0.54052\n",
      "[200]\ttrain-rmse:0.44837\tvalidation-rmse:0.53786\n",
      "[250]\ttrain-rmse:0.43039\tvalidation-rmse:0.53649\n",
      "[300]\ttrain-rmse:0.41390\tvalidation-rmse:0.53497\n",
      "[350]\ttrain-rmse:0.39894\tvalidation-rmse:0.53404\n",
      "[400]\ttrain-rmse:0.38450\tvalidation-rmse:0.53289\n",
      "[450]\ttrain-rmse:0.37063\tvalidation-rmse:0.53225\n",
      "[500]\ttrain-rmse:0.35788\tvalidation-rmse:0.53154\n",
      "[550]\ttrain-rmse:0.34609\tvalidation-rmse:0.53093\n",
      "[600]\ttrain-rmse:0.33426\tvalidation-rmse:0.53035\n",
      "[650]\ttrain-rmse:0.32303\tvalidation-rmse:0.52990\n",
      "[700]\ttrain-rmse:0.31249\tvalidation-rmse:0.52942\n",
      "[750]\ttrain-rmse:0.30224\tvalidation-rmse:0.52928\n",
      "[800]\ttrain-rmse:0.29260\tvalidation-rmse:0.52902\n",
      "[850]\ttrain-rmse:0.28317\tvalidation-rmse:0.52873\n",
      "[900]\ttrain-rmse:0.27473\tvalidation-rmse:0.52861\n",
      "[950]\ttrain-rmse:0.26607\tvalidation-rmse:0.52849\n",
      "[1000]\ttrain-rmse:0.25782\tvalidation-rmse:0.52812\n",
      "[1050]\ttrain-rmse:0.24975\tvalidation-rmse:0.52794\n",
      "[1100]\ttrain-rmse:0.24201\tvalidation-rmse:0.52782\n",
      "[1150]\ttrain-rmse:0.23428\tvalidation-rmse:0.52776\n",
      "[1200]\ttrain-rmse:0.22696\tvalidation-rmse:0.52767\n",
      "[1250]\ttrain-rmse:0.22022\tvalidation-rmse:0.52753\n",
      "[1300]\ttrain-rmse:0.21360\tvalidation-rmse:0.52732\n",
      "[1350]\ttrain-rmse:0.20696\tvalidation-rmse:0.52722\n",
      "[1388]\ttrain-rmse:0.20237\tvalidation-rmse:0.52721\n",
      "\n",
      "âœ“ Training complete\n",
      "  Best iteration: 1339\n",
      "  Best score: 0.5271740104486745\n",
      "\n",
      "âœ… XGBoost SMAPE: 38.9870%\n",
      "âœ… XGBoost working correctly!\n",
      "   Prediction range: [$0.85, $293.28]\n",
      "âœ“ Saved: xgb_model_final.json\n",
      "âœ“ Saved: xgb_model_final.pkl\n",
      "\n",
      "================================================================================\n",
      "ALL 3 MODELS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Individual Model Performance:\n",
      "   LightGBM:  37.7346%\n",
      "   CatBoost:  39.0249%\n",
      "   XGBoost:   38.9870%\n",
      "\n",
      "ðŸŽ¯ Expected Ensemble: ~36.23%\n",
      "\n",
      "âœ… ALL MODELS READY! Ready for Phase 2 (Ensemble & Inference)!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "UNIVERSAL XGBOOST - Works with Old and New Versions\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"UNIVERSAL XGBOOST TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check XGBoost version\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "\n",
    "# Data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full, y, test_size=0.2, random_state=42\n",
    ")\n",
    "idx_train, idx_val = train_test_split(\n",
    "    range(len(y)), test_size=0.2, random_state=42\n",
    ")\n",
    "y_val_orig = y_original[idx_val]\n",
    "\n",
    "print(f\"âœ“ Train: {len(X_train)}, Val: {len(X_val)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# USE NATIVE XGBOOST API (Most Compatible)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ“‹ Training XGBoost with native API...\")\n",
    "\n",
    "# Create DMatrix (XGBoost's internal data structure)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "# Parameters for native API\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'device': 'cuda',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.5,\n",
    "    'reg_lambda': 1.0,\n",
    "    'gamma': 0.1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train with early stopping (native API - always works)\n",
    "evals = [(dtrain, 'train'), (dval, 'validation')]\n",
    "evals_result = {}\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=2000,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=50,\n",
    "    evals_result=evals_result,\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Training complete\")\n",
    "print(f\"  Best iteration: {bst.best_iteration}\")\n",
    "print(f\"  Best score: {bst.best_score}\")\n",
    "\n",
    "# Make predictions\n",
    "xgb_pred_log = bst.predict(dval)\n",
    "xgb_pred = np.expm1(xgb_pred_log)\n",
    "\n",
    "# Calculate SMAPE\n",
    "xgb_smape = np.mean(2 * np.abs(xgb_pred - y_val_orig) / (np.abs(xgb_pred) + np.abs(y_val_orig))) * 100\n",
    "\n",
    "print(f\"\\nâœ… XGBoost SMAPE: {xgb_smape:.4f}%\")\n",
    "\n",
    "# Check if reasonable\n",
    "if xgb_smape < 100:\n",
    "    print(f\"âœ… XGBoost working correctly!\")\n",
    "    print(f\"   Prediction range: [${xgb_pred.min():.2f}, ${xgb_pred.max():.2f}]\")\n",
    "    \n",
    "    # Save the booster (native format)\n",
    "    bst.save_model('xgb_model_final.json')\n",
    "    \n",
    "    # Also save as pickle for compatibility\n",
    "    with open('xgb_model_final.pkl', 'wb') as f:\n",
    "        pickle.dump(bst, f)\n",
    "    \n",
    "    # Save params\n",
    "    with open('best_xgb_params.pkl', 'wb') as f:\n",
    "        pickle.dump(params, f)\n",
    "    \n",
    "    print(f\"âœ“ Saved: xgb_model_final.json\")\n",
    "    print(f\"âœ“ Saved: xgb_model_final.pkl\")\n",
    "else:\n",
    "    print(f\"âŒ XGBoost SMAPE still bad: {xgb_smape:.4f}%\")\n",
    "    print(f\"   Prediction range: [${xgb_pred.min():.2f}, ${xgb_pred.max():.2f}]\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "# Load CatBoost\n",
    "with open('cat_model_final.pkl', 'rb') as f:\n",
    "    cat_model = pickle.load(f)\n",
    "cat_pred = np.expm1(cat_model.predict(X_val))\n",
    "cat_smape = np.mean(2 * np.abs(cat_pred - y_val_orig) / (np.abs(cat_pred) + np.abs(y_val_orig))) * 100\n",
    "\n",
    "# Load LightGBM\n",
    "with open('lgb_metadata.pkl', 'rb') as f:\n",
    "    lgb_metadata = pickle.load(f)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL 3 MODELS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“Š Individual Model Performance:\")\n",
    "print(f\"   LightGBM:  {lgb_metadata['best_smape']:.4f}%\")\n",
    "print(f\"   CatBoost:  {cat_smape:.4f}%\")\n",
    "print(f\"   XGBoost:   {xgb_smape:.4f}%\")\n",
    "\n",
    "if xgb_smape < 100:\n",
    "    individual_scores = [lgb_metadata['best_smape'], cat_smape, xgb_smape]\n",
    "    best_individual = min(individual_scores)\n",
    "    estimated_ensemble = best_individual - 1.5\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Expected Ensemble: ~{estimated_ensemble:.2f}%\")\n",
    "    print(f\"\\nâœ… ALL MODELS READY! Ready for Phase 2 (Ensemble & Inference)!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ XGBoost failed - will use 2-model ensemble (LGB + CAT)\")\n",
    "    estimated_ensemble = min(lgb_metadata['best_smape'], cat_smape) - 1.0\n",
    "    print(f\"ðŸŽ¯ Expected 2-Model Ensemble: ~{estimated_ensemble:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbcb1a75-0156-4e8f-a5d9-5d665a538c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 2: ENSEMBLE & INFERENCE\n",
      "================================================================================\n",
      "\n",
      "[1/5] Loading trained models...\n",
      "âœ“ LightGBM loaded\n",
      "âœ“ CatBoost loaded\n",
      "âœ“ XGBoost loaded\n",
      "\n",
      "[2/5] Loading validation data...\n",
      "âœ“ Validation set: (15000, 2333)\n",
      "\n",
      "[3/5] Getting individual model predictions...\n",
      "âœ“ LightGBM: 37.7346%\n",
      "âœ“ CatBoost: 39.0249%\n",
      "âœ“ XGBoost: 38.9870%\n",
      "\n",
      "[4/5] Optimizing ensemble weights...\n",
      "\n",
      "âœ“ Optimal Weights:\n",
      "   LightGBM:  0.9793\n",
      "   CatBoost:  0.0000\n",
      "   XGBoost:   0.0207\n",
      "\n",
      "âœ“ Ensemble SMAPE: 37.7342%\n",
      "   Improvement over best: 0.0004%\n",
      "\n",
      "[5/5] Saving ensemble configuration...\n",
      "âœ“ Saved: ensemble_config.pkl\n",
      "\n",
      "================================================================================\n",
      "âœ… ENSEMBLE OPTIMIZATION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Final Validation Scores:\n",
      "   LightGBM:     37.7346%\n",
      "   CatBoost:     39.0249%\n",
      "   XGBoost:      38.9870%\n",
      "   ðŸ“ ENSEMBLE:  37.7342%\n",
      "\n",
      "ðŸš€ Next: Process test data and generate predictions\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PHASE 2 - CHUNK 1: Load Models & Optimize Ensemble Weights\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 2: ENSEMBLE & INFERENCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD ALL TRAINED MODELS\n",
    "# ============================================================================\n",
    "print(\"\\n[1/5] Loading trained models...\")\n",
    "\n",
    "# LightGBM\n",
    "with open('lgb_model_final.pkl', 'rb') as f:\n",
    "    lgb_model = pickle.load(f)\n",
    "print(\"âœ“ LightGBM loaded\")\n",
    "\n",
    "# CatBoost\n",
    "with open('cat_model_final.pkl', 'rb') as f:\n",
    "    cat_model = pickle.load(f)\n",
    "print(\"âœ“ CatBoost loaded\")\n",
    "\n",
    "# XGBoost (Booster object)\n",
    "with open('xgb_model_final.pkl', 'rb') as f:\n",
    "    xgb_model = pickle.load(f)\n",
    "print(\"âœ“ XGBoost loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD VALIDATION DATA\n",
    "# ============================================================================\n",
    "print(\"\\n[2/5] Loading validation data...\")\n",
    "\n",
    "X_full = np.load('X_full_features.npy')\n",
    "y = np.load('y_log_target.npy')\n",
    "y_original = np.load('y_original_prices.npy')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full, y, test_size=0.2, random_state=42\n",
    ")\n",
    "idx_train, idx_val = train_test_split(\n",
    "    range(len(y)), test_size=0.2, random_state=42\n",
    ")\n",
    "y_val_orig = y_original[idx_val]\n",
    "\n",
    "print(f\"âœ“ Validation set: {X_val.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# GET INDIVIDUAL PREDICTIONS\n",
    "# ============================================================================\n",
    "print(\"\\n[3/5] Getting individual model predictions...\")\n",
    "\n",
    "# LightGBM predictions\n",
    "lgb_pred = np.expm1(lgb_model.predict(X_val))\n",
    "lgb_smape = np.mean(2 * np.abs(lgb_pred - y_val_orig) / (np.abs(lgb_pred) + np.abs(y_val_orig))) * 100\n",
    "print(f\"âœ“ LightGBM: {lgb_smape:.4f}%\")\n",
    "\n",
    "# CatBoost predictions\n",
    "cat_pred = np.expm1(cat_model.predict(X_val))\n",
    "cat_smape = np.mean(2 * np.abs(cat_pred - y_val_orig) / (np.abs(cat_pred) + np.abs(y_val_orig))) * 100\n",
    "print(f\"âœ“ CatBoost: {cat_smape:.4f}%\")\n",
    "\n",
    "# XGBoost predictions (using DMatrix)\n",
    "dval = xgb.DMatrix(X_val)\n",
    "xgb_pred = np.expm1(xgb_model.predict(dval))\n",
    "xgb_smape = np.mean(2 * np.abs(xgb_pred - y_val_orig) / (np.abs(xgb_pred) + np.abs(y_val_orig))) * 100\n",
    "print(f\"âœ“ XGBoost: {xgb_smape:.4f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIMIZE ENSEMBLE WEIGHTS\n",
    "# ============================================================================\n",
    "print(\"\\n[4/5] Optimizing ensemble weights...\")\n",
    "\n",
    "def ensemble_smape(weights):\n",
    "    \"\"\"Calculate SMAPE for weighted ensemble\"\"\"\n",
    "    weights = weights / weights.sum()  # Normalize\n",
    "    ensemble_pred = (weights[0] * lgb_pred + \n",
    "                     weights[1] * cat_pred + \n",
    "                     weights[2] * xgb_pred)\n",
    "    smape = np.mean(2 * np.abs(ensemble_pred - y_val_orig) / \n",
    "                    (np.abs(ensemble_pred) + np.abs(y_val_orig))) * 100\n",
    "    return smape\n",
    "\n",
    "# Initial weights (equal)\n",
    "initial_weights = np.array([1/3, 1/3, 1/3])\n",
    "\n",
    "# Optimize\n",
    "result = minimize(\n",
    "    ensemble_smape,\n",
    "    initial_weights,\n",
    "    method='Nelder-Mead',\n",
    "    bounds=[(0, 1), (0, 1), (0, 1)],\n",
    "    options={'maxiter': 1000}\n",
    ")\n",
    "\n",
    "# Normalize optimal weights\n",
    "optimal_weights = result.x / result.x.sum()\n",
    "\n",
    "print(f\"\\nâœ“ Optimal Weights:\")\n",
    "print(f\"   LightGBM:  {optimal_weights[0]:.4f}\")\n",
    "print(f\"   CatBoost:  {optimal_weights[1]:.4f}\")\n",
    "print(f\"   XGBoost:   {optimal_weights[2]:.4f}\")\n",
    "\n",
    "# Calculate ensemble score\n",
    "ensemble_pred = (optimal_weights[0] * lgb_pred + \n",
    "                 optimal_weights[1] * cat_pred + \n",
    "                 optimal_weights[2] * xgb_pred)\n",
    "ensemble_smape_val = np.mean(2 * np.abs(ensemble_pred - y_val_orig) / \n",
    "                              (np.abs(ensemble_pred) + np.abs(y_val_orig))) * 100\n",
    "\n",
    "print(f\"\\nâœ“ Ensemble SMAPE: {ensemble_smape_val:.4f}%\")\n",
    "print(f\"   Improvement over best: {min(lgb_smape, cat_smape, xgb_smape) - ensemble_smape_val:.4f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE OPTIMAL WEIGHTS\n",
    "# ============================================================================\n",
    "print(\"\\n[5/5] Saving ensemble configuration...\")\n",
    "\n",
    "ensemble_config = {\n",
    "    'weights': optimal_weights,\n",
    "    'lgb_smape': lgb_smape,\n",
    "    'cat_smape': cat_smape,\n",
    "    'xgb_smape': xgb_smape,\n",
    "    'ensemble_smape': ensemble_smape_val\n",
    "}\n",
    "\n",
    "with open('ensemble_config.pkl', 'wb') as f:\n",
    "    pickle.dump(ensemble_config, f)\n",
    "\n",
    "print(f\"âœ“ Saved: ensemble_config.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… ENSEMBLE OPTIMIZATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“Š Final Validation Scores:\")\n",
    "print(f\"   LightGBM:     {lgb_smape:.4f}%\")\n",
    "print(f\"   CatBoost:     {cat_smape:.4f}%\")\n",
    "print(f\"   XGBoost:      {xgb_smape:.4f}%\")\n",
    "print(f\"   ðŸ“ ENSEMBLE:  {ensemble_smape_val:.4f}%\")\n",
    "\n",
    "print(f\"\\nðŸš€ Next: Process test data and generate predictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6213c75-110b-468e-ad5a-1bc219631e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "437fcbb2-725f-4ed4-a418-b0e3f1d841e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 2: COMPLETE TEST INFERENCE & SUBMISSION\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PART 1: TEST DATA PROCESSING\n",
      "================================================================================\n",
      "\n",
      "[1/8] Loading test data...\n",
      "âœ“ Loaded 75000 test samples\n",
      "\n",
      "[2/8] Loading metadata and encoders...\n",
      "âœ“ Metadata loaded\n",
      "  Expected dimensions:\n",
      "    Text: 1024\n",
      "    Title: 512\n",
      "    Image: 768\n",
      "\n",
      "[3/8] Parsing test embeddings with robust method...\n",
      "  [1/3] Parsing text embeddings (dim=1024)...\n",
      "    âœ“ Shape: (75000, 1024)\n",
      "  [2/3] Parsing title embeddings (dim=512)...\n",
      "    âœ“ Shape: (75000, 512)\n",
      "  [3/3] Parsing image embeddings (dim=768)...\n",
      "    âœ“ Shape: (75000, 768)\n",
      "    âš ï¸ 75000/75000 all-zero embeddings (100.0%)\n",
      "\n",
      "âœ“ All embeddings parsed with robust method\n",
      "\n",
      "[4/8] Extracting numeric features...\n",
      "âœ“ Numeric features: (75000, 10)\n",
      "\n",
      "[5/8] Extracting binary features...\n",
      "âœ“ Binary features: (75000, 5)\n",
      "\n",
      "[6/8] Extracting container features...\n",
      "âœ“ Container features: (75000, 10)\n",
      "\n",
      "[7/8] Encoding categorical features...\n",
      "âœ“ Categorical features: (75000, 4)\n",
      "\n",
      "[8/8] Combining all test features...\n",
      "\n",
      "âœ“ COMPLETE TEST FEATURE MATRIX: (75000, 2333)\n",
      "  Expected: 2333 features\n",
      "  âœ… Feature dimensions match!\n",
      "\n",
      "ðŸ“Š Quality Check:\n",
      "  Has NaN: False\n",
      "  Has Inf: False\n",
      "\n",
      "âœ… TEST DATA PROCESSING COMPLETE!\n",
      "\n",
      "================================================================================\n",
      "PART 2: ENSEMBLE PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "[1/5] Loading trained models...\n",
      "âœ“ Models loaded\n",
      "  Weights: LGB=0.9793, CAT=0.0000, XGB=0.0207\n",
      "\n",
      "[2/5] Generating predictions from each model...\n",
      "  â†’ LightGBM predicting...\n",
      "    Range: [$1.75, $211.50], Mean: $19.62\n",
      "  â†’ CatBoost predicting...\n",
      "    Range: [$2.06, $151.24], Mean: $19.75\n",
      "  â†’ XGBoost predicting...\n",
      "    Range: [$1.40, $176.24], Mean: $19.73\n",
      "\n",
      "[3/5] Creating weighted ensemble...\n",
      "âœ“ Ensemble predictions:\n",
      "  Min: $1.76\n",
      "  Max: $210.35\n",
      "  Mean: $19.63\n",
      "  Median: $15.45\n",
      "  Std: $16.21\n",
      "\n",
      "[4/5] Creating submission files...\n",
      "\n",
      "ðŸ“‹ Submission validation:\n",
      "  Rows: 75000\n",
      "  Columns: ['sample_id', 'price']\n",
      "  No missing values: True\n",
      "  No negative prices: True\n",
      "\n",
      "âœ… MAIN FILE: submission_ensemble.csv\n",
      "âœ… BACKUP: submission_ensemble_backup_20251012_215426.csv\n",
      "âœ… BREAKDOWN: predictions_breakdown.csv\n",
      "\n",
      "[5/5] Final verification...\n",
      "\n",
      "âœ“ Verified submission file:\n",
      "  Rows: 75000\n",
      "  File size: 146.5 KB\n",
      "\n",
      "ðŸ“‹ Sample predictions (first 10):\n",
      " sample_id     price\n",
      "    100179 15.094351\n",
      "    245611 14.350413\n",
      "    146263 14.193072\n",
      "     95658 15.853982\n",
      "     36806 34.624560\n",
      "    148239  7.215591\n",
      "     92659 15.957456\n",
      "      3780 16.373515\n",
      "    196940 23.264783\n",
      "     20472 11.505867\n",
      "\n",
      "================================================================================\n",
      "âœ… COMPLETE! SUBMISSION READY!\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Final Summary:\n",
      "  Total predictions: 75000\n",
      "  Expected SMAPE: 37.7342%\n",
      "\n",
      "ðŸ’¾ Files Created:\n",
      "  1. submission_ensemble.csv (MAIN)\n",
      "  2. submission_ensemble_backup_20251012_215426.csv (backup)\n",
      "  3. predictions_breakdown.csv (analysis)\n",
      "\n",
      "ðŸŽ¯ Model Contributions:\n",
      "  LightGBM:  97.9% (SMAPE: 37.7346%)\n",
      "  CatBoost:  0.0% (SMAPE: 39.0249%)\n",
      "  XGBoost:   2.1% (SMAPE: 38.9870%)\n",
      "\n",
      "ðŸš€ SUBMIT: submission_ensemble.csv\n",
      "================================================================================\n",
      "\n",
      "âœ… Metadata: submission_metadata.pkl\n",
      "\n",
      "ðŸŽ‰ ALL DONE!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PHASE 2: COMPLETE TEST INFERENCE & SUBMISSION\n",
    "Test Data Processing + Ensemble Predictions + File Saving\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import ast\n",
    "import xgboost as xgb\n",
    "from category_encoders import TargetEncoder\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 2: COMPLETE TEST INFERENCE & SUBMISSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# ROBUST EMBEDDING PARSER (Friend's Method)\n",
    "# ============================================================================\n",
    "\n",
    "def parse_single_embedding(embedding_str, expected_dim):\n",
    "    \"\"\"\n",
    "    Robust embedding parser with dimension handling and NaN/Inf cleanup\n",
    "    Based on friend's code\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle string format\n",
    "        if isinstance(embedding_str, str):\n",
    "            arr = np.array(ast.literal_eval(embedding_str), dtype=np.float32)\n",
    "            \n",
    "            # FIX 1: Handle dimension mismatch\n",
    "            if arr.shape[0] != expected_dim:\n",
    "                if arr.shape[0] < expected_dim:\n",
    "                    arr = np.pad(arr, (0, expected_dim - arr.shape[0]), mode='constant')\n",
    "                else:\n",
    "                    arr = arr[:expected_dim]\n",
    "            \n",
    "            # FIX 2: Replace NaN/Inf with 0.0\n",
    "            arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            return arr\n",
    "            \n",
    "        # Handle already parsed arrays\n",
    "        elif isinstance(embedding_str, (list, np.ndarray)):\n",
    "            arr = np.array(embedding_str, dtype=np.float32)\n",
    "            \n",
    "            # Handle dimension mismatch\n",
    "            if arr.shape[0] != expected_dim:\n",
    "                if arr.shape[0] < expected_dim:\n",
    "                    arr = np.pad(arr, (0, expected_dim - arr.shape[0]), mode='constant')\n",
    "                else:\n",
    "                    arr = arr[:expected_dim]\n",
    "            \n",
    "            # Replace NaN/Inf\n",
    "            arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            return arr\n",
    "            \n",
    "        # FIX 3: Fallback to zero vector\n",
    "        else:\n",
    "            return np.zeros(expected_dim, dtype=np.float32)\n",
    "            \n",
    "    except Exception as e:\n",
    "        # FIX 4: Catch ALL errors, return zero vector\n",
    "        return np.zeros(expected_dim, dtype=np.float32)\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: LOAD TEST DATA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 1: TEST DATA PROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n[1/8] Loading test data...\")\n",
    "\n",
    "TEST_FILE = 'test_all.csv'  # â† CHANGE THIS to your test file name\n",
    "\n",
    "test_df = pd.read_csv(TEST_FILE)\n",
    "print(f\"âœ“ Loaded {len(test_df)} test samples\")\n",
    "\n",
    "# Save sample_id for submission\n",
    "sample_ids = test_df['sample_id'].values\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD METADATA & ENCODERS\n",
    "# ============================================================================\n",
    "print(\"\\n[2/8] Loading metadata and encoders...\")\n",
    "\n",
    "with open('feature_metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "with open('target_encoder.pkl', 'rb') as f:\n",
    "    target_encoder = pickle.load(f)\n",
    "\n",
    "print(f\"âœ“ Metadata loaded\")\n",
    "\n",
    "# Get expected dimensions from metadata\n",
    "text_dim = metadata['text_dim']\n",
    "title_dim = metadata['title_dim']\n",
    "image_dim = metadata['image_dim']\n",
    "\n",
    "print(f\"  Expected dimensions:\")\n",
    "print(f\"    Text: {text_dim}\")\n",
    "print(f\"    Title: {title_dim}\")\n",
    "print(f\"    Image: {image_dim}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PARSE TEST EMBEDDINGS (ROBUST METHOD)\n",
    "# ============================================================================\n",
    "print(\"\\n[3/8] Parsing test embeddings with robust method...\")\n",
    "\n",
    "# Parse text embeddings\n",
    "print(f\"  [1/3] Parsing text embeddings (dim={text_dim})...\")\n",
    "test_text_list = []\n",
    "zero_count_text = 0\n",
    "\n",
    "for idx, emb in enumerate(test_df['text_finetuned_embeddings']):\n",
    "    parsed = parse_single_embedding(emb, text_dim)\n",
    "    test_text_list.append(parsed)\n",
    "    if np.all(parsed == 0):\n",
    "        zero_count_text += 1\n",
    "\n",
    "test_text_matrix = np.vstack(test_text_list)\n",
    "print(f\"    âœ“ Shape: {test_text_matrix.shape}\")\n",
    "if zero_count_text > 0:\n",
    "    print(f\"    âš ï¸ {zero_count_text}/{len(test_df)} all-zero embeddings ({zero_count_text/len(test_df)*100:.1f}%)\")\n",
    "\n",
    "# Parse title embeddings\n",
    "print(f\"  [2/3] Parsing title embeddings (dim={title_dim})...\")\n",
    "test_title_list = []\n",
    "zero_count_title = 0\n",
    "\n",
    "for idx, emb in enumerate(test_df['title_embeddings']):\n",
    "    parsed = parse_single_embedding(emb, title_dim)\n",
    "    test_title_list.append(parsed)\n",
    "    if np.all(parsed == 0):\n",
    "        zero_count_title += 1\n",
    "\n",
    "test_title_matrix = np.vstack(test_title_list)\n",
    "print(f\"    âœ“ Shape: {test_title_matrix.shape}\")\n",
    "if zero_count_title > 0:\n",
    "    print(f\"    âš ï¸ {zero_count_title}/{len(test_df)} all-zero embeddings ({zero_count_title/len(test_df)*100:.1f}%)\")\n",
    "\n",
    "# Parse image embeddings\n",
    "print(f\"  [3/3] Parsing image embeddings (dim={image_dim})...\")\n",
    "test_image_list = []\n",
    "zero_count_image = 0\n",
    "\n",
    "for idx, emb in enumerate(test_df['image_embedding']):\n",
    "    parsed = parse_single_embedding(emb, image_dim)\n",
    "    test_image_list.append(parsed)\n",
    "    if np.all(parsed == 0):\n",
    "        zero_count_image += 1\n",
    "\n",
    "test_image_matrix = np.vstack(test_image_list)\n",
    "print(f\"    âœ“ Shape: {test_image_matrix.shape}\")\n",
    "if zero_count_image > 0:\n",
    "    print(f\"    âš ï¸ {zero_count_image}/{len(test_df)} all-zero embeddings ({zero_count_image/len(test_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nâœ“ All embeddings parsed with robust method\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACT NUMERIC FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n[4/8] Extracting numeric features...\")\n",
    "\n",
    "numeric_features = metadata['numeric_features']\n",
    "numeric_data = []\n",
    "\n",
    "for col in numeric_features:\n",
    "    if col in test_df.columns:\n",
    "        test_df[col] = test_df[col].fillna(0)\n",
    "        numeric_data.append(test_df[col].values)\n",
    "    else:\n",
    "        print(f\"  âš ï¸ Missing: {col} (using zeros)\")\n",
    "        numeric_data.append(np.zeros(len(test_df)))\n",
    "\n",
    "if numeric_data:\n",
    "    test_numeric_matrix = np.column_stack(numeric_data)\n",
    "else:\n",
    "    test_numeric_matrix = np.zeros((len(test_df), 0))\n",
    "\n",
    "print(f\"âœ“ Numeric features: {test_numeric_matrix.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACT BINARY FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n[5/8] Extracting binary features...\")\n",
    "\n",
    "binary_features = metadata['binary_features']\n",
    "binary_data = []\n",
    "\n",
    "for col in binary_features:\n",
    "    if col in test_df.columns:\n",
    "        test_df[col] = pd.to_numeric(test_df[col], errors='coerce').fillna(0).astype(int)\n",
    "        binary_data.append(test_df[col].values)\n",
    "    else:\n",
    "        print(f\"  âš ï¸ Missing: {col} (using zeros)\")\n",
    "        binary_data.append(np.zeros(len(test_df)))\n",
    "\n",
    "if binary_data:\n",
    "    test_binary_matrix = np.column_stack(binary_data)\n",
    "else:\n",
    "    test_binary_matrix = np.zeros((len(test_df), 0))\n",
    "\n",
    "print(f\"âœ“ Binary features: {test_binary_matrix.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACT CONTAINER FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n[6/8] Extracting container features...\")\n",
    "\n",
    "container_features = metadata['container_features']\n",
    "container_data = []\n",
    "\n",
    "for col in container_features:\n",
    "    if col in test_df.columns:\n",
    "        test_df[col] = test_df[col].fillna(0).astype(int)\n",
    "        container_data.append(test_df[col].values)\n",
    "    else:\n",
    "        print(f\"  âš ï¸ Missing: {col} (using zeros)\")\n",
    "        container_data.append(np.zeros(len(test_df)))\n",
    "\n",
    "if container_data:\n",
    "    test_container_matrix = np.column_stack(container_data)\n",
    "else:\n",
    "    test_container_matrix = np.zeros((len(test_df), 0))\n",
    "\n",
    "print(f\"âœ“ Container features: {test_container_matrix.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ENCODE CATEGORICAL FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n[7/8] Encoding categorical features...\")\n",
    "\n",
    "categorical_features = metadata['categorical_features']\n",
    "\n",
    "if categorical_features and target_encoder:\n",
    "    test_categorical_encoded = target_encoder.transform(test_df[categorical_features])\n",
    "    test_categorical_matrix = test_categorical_encoded.values\n",
    "    print(f\"âœ“ Categorical features: {test_categorical_matrix.shape}\")\n",
    "else:\n",
    "    test_categorical_matrix = np.zeros((len(test_df), 0))\n",
    "    print(f\"  No categorical features to encode\")\n",
    "\n",
    "# ============================================================================\n",
    "# COMBINE ALL FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n[8/8] Combining all test features...\")\n",
    "\n",
    "X_test = np.hstack([\n",
    "    test_text_matrix,\n",
    "    test_title_matrix,\n",
    "    test_image_matrix,\n",
    "    test_numeric_matrix,\n",
    "    test_binary_matrix,\n",
    "    test_container_matrix,\n",
    "    test_categorical_matrix\n",
    "])\n",
    "\n",
    "print(f\"\\nâœ“ COMPLETE TEST FEATURE MATRIX: {X_test.shape}\")\n",
    "print(f\"  Expected: {metadata['total_features']} features\")\n",
    "\n",
    "if X_test.shape[1] != metadata['total_features']:\n",
    "    print(f\"  âš ï¸ WARNING: Feature mismatch!\")\n",
    "    print(f\"    Expected: {metadata['total_features']}\")\n",
    "    print(f\"    Got: {X_test.shape[1]}\")\n",
    "else:\n",
    "    print(f\"  âœ… Feature dimensions match!\")\n",
    "\n",
    "# Quality check\n",
    "print(f\"\\nðŸ“Š Quality Check:\")\n",
    "print(f\"  Has NaN: {np.isnan(X_test).any()}\")\n",
    "print(f\"  Has Inf: {np.isinf(X_test).any()}\")\n",
    "\n",
    "print(\"\\nâœ… TEST DATA PROCESSING COMPLETE!\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: LOAD MODELS & GENERATE PREDICTIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2: ENSEMBLE PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n[1/5] Loading trained models...\")\n",
    "\n",
    "with open('lgb_model_final.pkl', 'rb') as f:\n",
    "    lgb_model = pickle.load(f)\n",
    "\n",
    "with open('cat_model_final.pkl', 'rb') as f:\n",
    "    cat_model = pickle.load(f)\n",
    "\n",
    "with open('xgb_model_final.pkl', 'rb') as f:\n",
    "    xgb_model = pickle.load(f)\n",
    "\n",
    "with open('ensemble_config.pkl', 'rb') as f:\n",
    "    ensemble_config = pickle.load(f)\n",
    "\n",
    "optimal_weights = ensemble_config['weights']\n",
    "\n",
    "print(f\"âœ“ Models loaded\")\n",
    "print(f\"  Weights: LGB={optimal_weights[0]:.4f}, CAT={optimal_weights[1]:.4f}, XGB={optimal_weights[2]:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATE PREDICTIONS FROM EACH MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n[2/5] Generating predictions from each model...\")\n",
    "\n",
    "# LightGBM\n",
    "print(\"  â†’ LightGBM predicting...\")\n",
    "lgb_test_pred_log = lgb_model.predict(X_test)\n",
    "lgb_test_pred = np.expm1(lgb_test_pred_log)\n",
    "print(f\"    Range: [${lgb_test_pred.min():.2f}, ${lgb_test_pred.max():.2f}], Mean: ${lgb_test_pred.mean():.2f}\")\n",
    "\n",
    "# CatBoost\n",
    "print(\"  â†’ CatBoost predicting...\")\n",
    "cat_test_pred_log = cat_model.predict(X_test)\n",
    "cat_test_pred = np.expm1(cat_test_pred_log)\n",
    "print(f\"    Range: [${cat_test_pred.min():.2f}, ${cat_test_pred.max():.2f}], Mean: ${cat_test_pred.mean():.2f}\")\n",
    "\n",
    "# XGBoost\n",
    "print(\"  â†’ XGBoost predicting...\")\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "xgb_test_pred_log = xgb_model.predict(dtest)\n",
    "xgb_test_pred = np.expm1(xgb_test_pred_log)\n",
    "print(f\"    Range: [${xgb_test_pred.min():.2f}, ${xgb_test_pred.max():.2f}], Mean: ${xgb_test_pred.mean():.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE ENSEMBLE\n",
    "# ============================================================================\n",
    "print(\"\\n[3/5] Creating weighted ensemble...\")\n",
    "\n",
    "ensemble_predictions = (\n",
    "    optimal_weights[0] * lgb_test_pred +\n",
    "    optimal_weights[1] * cat_test_pred +\n",
    "    optimal_weights[2] * xgb_test_pred\n",
    ")\n",
    "\n",
    "# Clip extreme values\n",
    "ensemble_predictions = np.clip(ensemble_predictions, 0.5, 500.0)\n",
    "\n",
    "print(f\"âœ“ Ensemble predictions:\")\n",
    "print(f\"  Min: ${ensemble_predictions.min():.2f}\")\n",
    "print(f\"  Max: ${ensemble_predictions.max():.2f}\")\n",
    "print(f\"  Mean: ${ensemble_predictions.mean():.2f}\")\n",
    "print(f\"  Median: ${np.median(ensemble_predictions):.2f}\")\n",
    "print(f\"  Std: ${ensemble_predictions.std():.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE SUBMISSION FILES\n",
    "# ============================================================================\n",
    "print(\"\\n[4/5] Creating submission files...\")\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'sample_id': sample_ids,\n",
    "    'price': ensemble_predictions\n",
    "})\n",
    "\n",
    "# Verify format\n",
    "print(f\"\\nðŸ“‹ Submission validation:\")\n",
    "print(f\"  Rows: {len(submission)}\")\n",
    "print(f\"  Columns: {list(submission.columns)}\")\n",
    "print(f\"  No missing values: {not submission.isnull().any().any()}\")\n",
    "print(f\"  No negative prices: {(submission['price'] >= 0).all()}\")\n",
    "\n",
    "# Save main submission\n",
    "submission.to_csv('submission_ensemble.csv', index=False)\n",
    "print(f\"\\nâœ… MAIN FILE: submission_ensemble.csv\")\n",
    "\n",
    "# Save timestamped backup\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "backup_filename = f'submission_ensemble_backup_{timestamp}.csv'\n",
    "submission.to_csv(backup_filename, index=False)\n",
    "print(f\"âœ… BACKUP: {backup_filename}\")\n",
    "\n",
    "# Save individual predictions\n",
    "individual_predictions = pd.DataFrame({\n",
    "    'sample_id': sample_ids,\n",
    "    'lgb_pred': lgb_test_pred,\n",
    "    'cat_pred': cat_test_pred,\n",
    "    'xgb_pred': xgb_test_pred,\n",
    "    'ensemble_pred': ensemble_predictions\n",
    "})\n",
    "individual_predictions.to_csv('predictions_breakdown.csv', index=False)\n",
    "print(f\"âœ… BREAKDOWN: predictions_breakdown.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL VERIFICATION & SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n[5/5] Final verification...\")\n",
    "\n",
    "verify_df = pd.read_csv('submission_ensemble.csv')\n",
    "print(f\"\\nâœ“ Verified submission file:\")\n",
    "print(f\"  Rows: {len(verify_df)}\")\n",
    "print(f\"  File size: {len(verify_df) * 2 / 1024:.1f} KB\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Sample predictions (first 10):\")\n",
    "print(submission.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… COMPLETE! SUBMISSION READY!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“Š Final Summary:\")\n",
    "print(f\"  Total predictions: {len(ensemble_predictions)}\")\n",
    "print(f\"  Expected SMAPE: {ensemble_config['ensemble_smape']:.4f}%\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ Files Created:\")\n",
    "print(f\"  1. submission_ensemble.csv (MAIN)\")\n",
    "print(f\"  2. {backup_filename} (backup)\")\n",
    "print(f\"  3. predictions_breakdown.csv (analysis)\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Model Contributions:\")\n",
    "print(f\"  LightGBM:  {optimal_weights[0]*100:.1f}% (SMAPE: {ensemble_config['lgb_smape']:.4f}%)\")\n",
    "print(f\"  CatBoost:  {optimal_weights[1]*100:.1f}% (SMAPE: {ensemble_config['cat_smape']:.4f}%)\")\n",
    "print(f\"  XGBoost:   {optimal_weights[2]*100:.1f}% (SMAPE: {ensemble_config['xgb_smape']:.4f}%)\")\n",
    "\n",
    "print(f\"\\nðŸš€ SUBMIT: submission_ensemble.csv\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save metadata\n",
    "final_metadata = {\n",
    "    'timestamp': timestamp,\n",
    "    'num_predictions': len(ensemble_predictions),\n",
    "    'ensemble_weights': optimal_weights.tolist(),\n",
    "    'validation_smape': ensemble_config['ensemble_smape'],\n",
    "    'individual_smapes': {\n",
    "        'lgb': ensemble_config['lgb_smape'],\n",
    "        'cat': ensemble_config['cat_smape'],\n",
    "        'xgb': ensemble_config['xgb_smape']\n",
    "    },\n",
    "    'prediction_stats': {\n",
    "        'min': float(ensemble_predictions.min()),\n",
    "        'max': float(ensemble_predictions.max()),\n",
    "        'mean': float(ensemble_predictions.mean()),\n",
    "        'median': float(np.median(ensemble_predictions)),\n",
    "        'std': float(ensemble_predictions.std())\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('submission_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(final_metadata, f)\n",
    "\n",
    "print(f\"\\nâœ… Metadata: submission_metadata.pkl\")\n",
    "print(\"\\nðŸŽ‰ ALL DONE!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca2139b-75c2-4287-ad19-453f8cc196b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
