{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8220d513-bdf9-4282-8d24-91c22fe1a879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CROSS-ATTENTION MULTIMODAL FUSION\n",
      "================================================================================\n",
      "\n",
      "✓ Device: cuda\n",
      "✓ Cross-Attention Fusion model defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CHUNK 1: Cross-Attention Fusion Network for Multimodal Embeddings\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CROSS-ATTENTION MULTIMODAL FUSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configuration\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\n✓ Device: {DEVICE}\")\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head cross-attention for fusing different modality embeddings\n",
    "    Query from one modality attends to Key-Value from other modalities\n",
    "    \"\"\"\n",
    "    def __init__(self, query_dim, kv_dim, embed_dim=256, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        # Linear projections\n",
    "        self.query_proj = nn.Linear(query_dim, embed_dim)\n",
    "        self.key_proj = nn.Linear(kv_dim, embed_dim)\n",
    "        self.value_proj = nn.Linear(kv_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "    def forward(self, query, key_value):\n",
    "        \"\"\"\n",
    "        query: (batch, query_dim)\n",
    "        key_value: (batch, kv_dim)\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Project and reshape: (batch, embed_dim) -> (batch, num_heads, head_dim)\n",
    "        Q = self.query_proj(query).view(batch_size, self.num_heads, self.head_dim)\n",
    "        K = self.key_proj(key_value).view(batch_size, self.num_heads, self.head_dim)\n",
    "        V = self.value_proj(key_value).view(batch_size, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Attention scores: (batch, num_heads, head_dim) x (batch, num_heads, head_dim)\n",
    "        # -> (batch, num_heads)\n",
    "        attn_scores = torch.sum(Q * K, dim=-1) * self.scale\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention: (batch, num_heads) x (batch, num_heads, head_dim)\n",
    "        # -> (batch, num_heads, head_dim)\n",
    "        attn_output = attn_weights.unsqueeze(-1) * V\n",
    "        \n",
    "        # Concatenate heads: (batch, num_heads, head_dim) -> (batch, embed_dim)\n",
    "        attn_output = attn_output.view(batch_size, self.embed_dim)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.out_proj(attn_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete cross-attention fusion for text, title, and image embeddings\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        text_dim=1024, \n",
    "        title_dim=512, \n",
    "        image_dim=768,\n",
    "        numeric_dim=3,\n",
    "        fusion_dim=256,\n",
    "        num_heads=8,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input projections to common dimension\n",
    "        self.text_proj = nn.Linear(text_dim, fusion_dim)\n",
    "        self.title_proj = nn.Linear(title_dim, fusion_dim)\n",
    "        self.image_proj = nn.Linear(image_dim, fusion_dim)\n",
    "        self.numeric_proj = nn.Linear(numeric_dim, fusion_dim)\n",
    "        \n",
    "        # Cross-attention layers: Each modality attends to others\n",
    "        # Text attends to Title\n",
    "        self.text_to_title_attn = MultiHeadCrossAttention(\n",
    "            fusion_dim, fusion_dim, fusion_dim, num_heads, dropout\n",
    "        )\n",
    "        # Text attends to Image\n",
    "        self.text_to_image_attn = MultiHeadCrossAttention(\n",
    "            fusion_dim, fusion_dim, fusion_dim, num_heads, dropout\n",
    "        )\n",
    "        \n",
    "        # Title attends to Text\n",
    "        self.title_to_text_attn = MultiHeadCrossAttention(\n",
    "            fusion_dim, fusion_dim, fusion_dim, num_heads, dropout\n",
    "        )\n",
    "        # Title attends to Image\n",
    "        self.title_to_image_attn = MultiHeadCrossAttention(\n",
    "            fusion_dim, fusion_dim, fusion_dim, num_heads, dropout\n",
    "        )\n",
    "        \n",
    "        # Image attends to Text\n",
    "        self.image_to_text_attn = MultiHeadCrossAttention(\n",
    "            fusion_dim, fusion_dim, fusion_dim, num_heads, dropout\n",
    "        )\n",
    "        # Image attends to Title\n",
    "        self.image_to_title_attn = MultiHeadCrossAttention(\n",
    "            fusion_dim, fusion_dim, fusion_dim, num_heads, dropout\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.text_norm = nn.LayerNorm(fusion_dim)\n",
    "        self.title_norm = nn.LayerNorm(fusion_dim)\n",
    "        self.image_norm = nn.LayerNorm(fusion_dim)\n",
    "        self.numeric_norm = nn.LayerNorm(fusion_dim)\n",
    "        \n",
    "        # Fusion layers\n",
    "        self.fusion_mlp = nn.Sequential(\n",
    "            nn.Linear(fusion_dim * 4, fusion_dim * 2),\n",
    "            nn.LayerNorm(fusion_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(fusion_dim * 2, fusion_dim),\n",
    "            nn.LayerNorm(fusion_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Prediction head\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, text_emb, title_emb, image_emb, numeric_feat):\n",
    "        \"\"\"\n",
    "        text_emb: (batch, text_dim)\n",
    "        title_emb: (batch, title_dim)\n",
    "        image_emb: (batch, image_dim)\n",
    "        numeric_feat: (batch, numeric_dim)\n",
    "        \"\"\"\n",
    "        # Project all to common dimension\n",
    "        text = self.text_proj(text_emb)\n",
    "        title = self.title_proj(title_emb)\n",
    "        image = self.image_proj(image_emb)\n",
    "        numeric = self.numeric_proj(numeric_feat)\n",
    "        \n",
    "        # Cross-attention fusion\n",
    "        # Text enriched by Title and Image\n",
    "        text_title, _ = self.text_to_title_attn(text, title)\n",
    "        text_image, _ = self.text_to_image_attn(text, image)\n",
    "        text_fused = self.text_norm(text + text_title + text_image)\n",
    "        \n",
    "        # Title enriched by Text and Image\n",
    "        title_text, _ = self.title_to_text_attn(title, text)\n",
    "        title_image, _ = self.title_to_image_attn(title, image)\n",
    "        title_fused = self.title_norm(title + title_text + title_image)\n",
    "        \n",
    "        # Image enriched by Text and Title\n",
    "        image_text, _ = self.image_to_text_attn(image, text)\n",
    "        image_title, _ = self.image_to_title_attn(image, title)\n",
    "        image_fused = self.image_norm(image + image_text + image_title)\n",
    "        \n",
    "        # Numeric normalized\n",
    "        numeric_fused = self.numeric_norm(numeric)\n",
    "        \n",
    "        # Concatenate all fused representations\n",
    "        fused = torch.cat([text_fused, title_fused, image_fused, numeric_fused], dim=1)\n",
    "        \n",
    "        # Final fusion MLP\n",
    "        fused = self.fusion_mlp(fused)\n",
    "        \n",
    "        # Predict price (in log space)\n",
    "        output = self.regressor(fused)\n",
    "        \n",
    "        return output.squeeze(-1)\n",
    "\n",
    "\n",
    "print(\"✓ Cross-Attention Fusion model defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27f2c02f-7b5e-4836-8046-daab53b6f298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset class defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CHUNK 2: Dataset and DataLoader\n",
    "\"\"\"\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, text_emb, title_emb, image_emb, numeric_feat, prices):\n",
    "        self.text_emb = torch.FloatTensor(text_emb)\n",
    "        self.title_emb = torch.FloatTensor(title_emb)\n",
    "        self.image_emb = torch.FloatTensor(image_emb)\n",
    "        self.numeric_feat = torch.FloatTensor(numeric_feat)\n",
    "        self.prices = torch.FloatTensor(prices)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.prices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.text_emb[idx],\n",
    "            self.title_emb[idx],\n",
    "            self.image_emb[idx],\n",
    "            self.numeric_feat[idx],\n",
    "            self.prices[idx]\n",
    "        )\n",
    "\n",
    "print(\"✓ Dataset class defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d067567-71a1-4de1-ab34-a47c6ff8fbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING DATA\n",
      "================================================================================\n",
      "✓ Loaded 75000 samples\n",
      "\n",
      "Parsing embeddings...\n",
      "✓ Text embeddings: (75000, 1024)\n",
      "✓ Title embeddings: (75000, 512)\n",
      "✓ Image embeddings: (75000, 768)\n",
      "✓ Numeric features: (75000, 3)\n",
      "✓ Target: (75000,)\n",
      "\n",
      "✓ Train: 60000, Val: 15000\n",
      "✓ DataLoaders created (batch size: 64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CHUNK 3: Load and Prepare Data\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load your data\n",
    "train_df = pd.read_csv('filtered_final_image_train.csv')\n",
    "\n",
    "print(f\"✓ Loaded {len(train_df)} samples\")\n",
    "\n",
    "# Parse embeddings (same as before)\n",
    "import ast\n",
    "\n",
    "def parse_embedding(emb_str):\n",
    "    if isinstance(emb_str, str):\n",
    "        try:\n",
    "            return np.array(ast.literal_eval(emb_str), dtype=np.float32)\n",
    "        except:\n",
    "            emb = emb_str.replace('[', '').replace(']', '').replace(',', ' ').split()\n",
    "            return np.array([float(x) for x in emb if x], dtype=np.float32)\n",
    "    elif isinstance(emb_str, (list, np.ndarray)):\n",
    "        return np.array(emb_str, dtype=np.float32)\n",
    "    return np.zeros(1, dtype=np.float32)\n",
    "\n",
    "print(\"\\nParsing embeddings...\")\n",
    "train_df['text_finetuned_embeddings'] = train_df['text_finetuned_embeddings'].apply(parse_embedding)\n",
    "train_df['title_embeddings'] = train_df['title_embeddings'].apply(parse_embedding)\n",
    "train_df['image_embedding'] = train_df['image_embedding'].apply(parse_embedding)\n",
    "\n",
    "# Extract matrices\n",
    "text_matrix = np.stack(train_df['text_finetuned_embeddings'].values)\n",
    "title_matrix = np.stack(train_df['title_embeddings'].values)\n",
    "image_matrix = np.stack(train_df['image_embedding'].values)\n",
    "\n",
    "print(f\"✓ Text embeddings: {text_matrix.shape}\")\n",
    "print(f\"✓ Title embeddings: {title_matrix.shape}\")\n",
    "print(f\"✓ Image embeddings: {image_matrix.shape}\")\n",
    "\n",
    "# Numeric features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_unit = LabelEncoder()\n",
    "train_df['unit_encoded'] = le_unit.fit_transform(train_df['unit_raw'].fillna('unknown'))\n",
    "train_df['value_raw'] = train_df['value_raw'].fillna(0)\n",
    "train_df['pack_count'] = train_df['pack_count'].fillna(1)\n",
    "\n",
    "numeric_features = train_df[['value_raw', 'pack_count', 'unit_encoded']].values\n",
    "print(f\"✓ Numeric features: {numeric_features.shape}\")\n",
    "\n",
    "# Target (log-transformed)\n",
    "prices = np.log1p(train_df['price'].values)\n",
    "print(f\"✓ Target: {prices.shape}\")\n",
    "\n",
    "# Train-val split\n",
    "X_text_train, X_text_val, \\\n",
    "X_title_train, X_title_val, \\\n",
    "X_image_train, X_image_val, \\\n",
    "X_numeric_train, X_numeric_val, \\\n",
    "y_train, y_val = train_test_split(\n",
    "    text_matrix, title_matrix, image_matrix, numeric_features, prices,\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Train: {len(y_train)}, Val: {len(y_val)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MultimodalDataset(\n",
    "    X_text_train, X_title_train, X_image_train, X_numeric_train, y_train\n",
    ")\n",
    "val_dataset = MultimodalDataset(\n",
    "    X_text_val, X_title_val, X_image_val, X_numeric_val, y_val\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"✓ DataLoaders created (batch size: {BATCH_SIZE})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63ca55c2-1e7e-4a0d-85a5-d9c8bdd404c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "✓ Model initialized\n",
      "  Parameters: 2,871,937\n",
      "\n",
      "Training for 50 epochs...\n",
      "Epoch 1/50 | Train Loss: 0.2977 | Val Loss: 0.2400\n",
      "  ✓ New best model saved!\n",
      "Epoch 2/50 | Train Loss: 0.2365 | Val Loss: 0.2265\n",
      "  ✓ New best model saved!\n",
      "Epoch 3/50 | Train Loss: 0.2191 | Val Loss: 0.2158\n",
      "  ✓ New best model saved!\n",
      "Epoch 4/50 | Train Loss: 0.2031 | Val Loss: 0.2128\n",
      "  ✓ New best model saved!\n",
      "Epoch 5/50 | Train Loss: 0.1902 | Val Loss: 0.2072\n",
      "  ✓ New best model saved!\n",
      "Epoch 6/50 | Train Loss: 0.1771 | Val Loss: 0.2063\n",
      "  ✓ New best model saved!\n",
      "Epoch 7/50 | Train Loss: 0.1649 | Val Loss: 0.2029\n",
      "  ✓ New best model saved!\n",
      "Epoch 8/50 | Train Loss: 0.1534 | Val Loss: 0.2007\n",
      "  ✓ New best model saved!\n",
      "Epoch 9/50 | Train Loss: 0.1422 | Val Loss: 0.2017\n",
      "Epoch 10/50 | Train Loss: 0.1324 | Val Loss: 0.1990\n",
      "  ✓ New best model saved!\n",
      "Epoch 11/50 | Train Loss: 0.1242 | Val Loss: 0.2109\n",
      "Epoch 12/50 | Train Loss: 0.1153 | Val Loss: 0.1969\n",
      "  ✓ New best model saved!\n",
      "Epoch 13/50 | Train Loss: 0.1083 | Val Loss: 0.2027\n",
      "Epoch 14/50 | Train Loss: 0.1009 | Val Loss: 0.2051\n",
      "Epoch 15/50 | Train Loss: 0.0937 | Val Loss: 0.1995\n",
      "Epoch 16/50 | Train Loss: 0.0885 | Val Loss: 0.2086\n",
      "Epoch 17/50 | Train Loss: 0.0834 | Val Loss: 0.2072\n",
      "Epoch 18/50 | Train Loss: 0.0780 | Val Loss: 0.2026\n",
      "Epoch 19/50 | Train Loss: 0.0738 | Val Loss: 0.2043\n",
      "Epoch 20/50 | Train Loss: 0.0689 | Val Loss: 0.2010\n",
      "Epoch 21/50 | Train Loss: 0.0655 | Val Loss: 0.2051\n",
      "Epoch 22/50 | Train Loss: 0.0612 | Val Loss: 0.2058\n",
      "\n",
      "Early stopping at epoch 22\n",
      "\n",
      "✓ Training complete!\n",
      "  Best validation loss: 0.1969\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CHUNK 4: Train Cross-Attention Model\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize model\n",
    "model = CrossAttentionFusion(\n",
    "    text_dim=text_matrix.shape[1],\n",
    "    title_dim=title_matrix.shape[1],\n",
    "    image_dim=image_matrix.shape[1],\n",
    "    numeric_dim=numeric_features.shape[1],\n",
    "    fusion_dim=256,\n",
    "    num_heads=8,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"✓ Model initialized\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.HuberLoss()  # Robust to outliers\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "# Training loop\n",
    "NUM_EPOCHS = 50\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"\\nTraining for {NUM_EPOCHS} epochs...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for text, title, image, numeric, target in train_loader:\n",
    "        text = text.to(DEVICE)\n",
    "        title = title.to(DEVICE)\n",
    "        image = image.to(DEVICE)\n",
    "        numeric = numeric.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(text, title, image, numeric)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text, title, image, numeric, target in val_loader:\n",
    "            text = text.to(DEVICE)\n",
    "            title = title.to(DEVICE)\n",
    "            image = image.to(DEVICE)\n",
    "            numeric = numeric.to(DEVICE)\n",
    "            target = target.to(DEVICE)\n",
    "            \n",
    "            output = model(text, title, image, numeric)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_cross_attention_model.pt')\n",
    "        print(f\"  ✓ New best model saved!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n✓ Training complete!\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44ccdb35-e026-42a6-846f-b10a568b0e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION\n",
      "================================================================================\n",
      "\n",
      "✓ Cross-Attention Fusion Results:\n",
      "  SMAPE: 47.7994%\n",
      "  MAE: $10.90\n",
      "  RMSE: $31.67\n",
      "\n",
      "Sample predictions:\n",
      "      Actual  Predicted      Error\n",
      "0  12.195001   4.853241   7.341760\n",
      "1  38.540001  33.252224   5.287777\n",
      "2  17.859999   6.750907  11.109092\n",
      "3   2.940000   3.853761   0.913761\n",
      "4  25.990000  29.317368   3.327368\n",
      "5  41.510002  50.322361   8.812359\n",
      "6  59.200008  64.237213   5.037205\n",
      "7  24.920000  31.176380   6.256380\n",
      "8  82.009995  58.449310  23.560684\n",
      "9  19.285000  12.616335   6.668665\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CHUNK 5: Evaluate on Validation Set\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_cross_attention_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "# Get predictions\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text, title, image, numeric, target in val_loader:\n",
    "        text = text.to(DEVICE)\n",
    "        title = title.to(DEVICE)\n",
    "        image = image.to(DEVICE)\n",
    "        numeric = numeric.to(DEVICE)\n",
    "        \n",
    "        output = model(text, title, image, numeric)\n",
    "        \n",
    "        all_preds.append(output.cpu().numpy())\n",
    "        all_targets.append(target.cpu().numpy())\n",
    "\n",
    "# Concatenate\n",
    "y_pred_log = np.concatenate(all_preds)\n",
    "y_true_log = np.concatenate(all_targets)\n",
    "\n",
    "# Convert back from log space\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_true = np.expm1(y_true_log)\n",
    "\n",
    "# Calculate SMAPE\n",
    "smape = np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true))) * 100\n",
    "\n",
    "# Calculate MAE and RMSE\n",
    "mae = np.mean(np.abs(y_pred - y_true))\n",
    "rmse = np.sqrt(np.mean((y_pred - y_true) ** 2))\n",
    "\n",
    "print(f\"\\n✓ Cross-Attention Fusion Results:\")\n",
    "print(f\"  SMAPE: {smape:.4f}%\")\n",
    "print(f\"  MAE: ${mae:.2f}\")\n",
    "print(f\"  RMSE: ${rmse:.2f}\")\n",
    "\n",
    "print(f\"\\nSample predictions:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Actual': y_true[:10],\n",
    "    'Predicted': y_pred[:10],\n",
    "    'Error': np.abs(y_pred[:10] - y_true[:10])\n",
    "})\n",
    "print(comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d3eb27-45dc-4fe8-93c8-f83bc8649f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INFERENCE ON TEST DATA\n",
      "================================================================================\n",
      "\n",
      "[1/6] Loading test data...\n",
      "✓ Loaded 75000 test samples\n",
      "  Columns: ['sample_id', 'title_embeddings', 'image_embedding', 'text_finetuned_embeddings', 'value_raw', 'unit_raw', 'pack_count']\n",
      "\n",
      "[2/6] Parsing test embeddings...\n",
      "  Parsing text_finetuned_embeddings...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CHUNK 6: Inference on Test Data\n",
    "Generate predictions using trained Cross-Attention model\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INFERENCE ON TEST DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: LOAD TEST DATA\n",
    "# ============================================================================\n",
    "print(\"\\n[1/6] Loading test data...\")\n",
    "\n",
    "TEST_FILE = 'filtered_final_image_test.csv'  # ← CHANGE THIS to your test file name\n",
    "test_df = pd.read_csv(TEST_FILE)\n",
    "\n",
    "print(f\"✓ Loaded {len(test_df)} test samples\")\n",
    "print(f\"  Columns: {test_df.columns.tolist()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: PARSE TEST EMBEDDINGS\n",
    "# ============================================================================\n",
    "print(\"\\n[2/6] Parsing test embeddings...\")\n",
    "\n",
    "# Use same parsing function as training\n",
    "def parse_embedding(emb_str):\n",
    "    if isinstance(emb_str, str):\n",
    "        try:\n",
    "            return np.array(ast.literal_eval(emb_str), dtype=np.float32)\n",
    "        except:\n",
    "            emb = emb_str.replace('[', '').replace(']', '').replace(',', ' ').split()\n",
    "            return np.array([float(x) for x in emb if x], dtype=np.float32)\n",
    "    elif isinstance(emb_str, (list, np.ndarray)):\n",
    "        return np.array(emb_str, dtype=np.float32)\n",
    "    return np.zeros(1, dtype=np.float32)\n",
    "\n",
    "# Parse all three embeddings\n",
    "print(\"  Parsing text_finetuned_embeddings...\")\n",
    "test_df['text_finetuned_embeddings'] = test_df['text_finetuned_embeddings'].apply(parse_embedding)\n",
    "\n",
    "print(\"  Parsing title_embeddings...\")\n",
    "test_df['title_embeddings'] = test_df['title_embeddings'].apply(parse_embedding)\n",
    "\n",
    "print(\"  Parsing image_embedding...\")\n",
    "test_df['image_embedding'] = test_df['image_embedding'].apply(parse_embedding)\n",
    "\n",
    "# Extract matrices\n",
    "test_text_matrix = np.stack(test_df['text_finetuned_embeddings'].values)\n",
    "test_title_matrix = np.stack(test_df['title_embeddings'].values)\n",
    "test_image_matrix = np.stack(test_df['image_embedding'].values)\n",
    "\n",
    "print(f\"\\n✓ Embedding matrices:\")\n",
    "print(f\"  Text: {test_text_matrix.shape}\")\n",
    "print(f\"  Title: {test_title_matrix.shape}\")\n",
    "print(f\"  Image: {test_image_matrix.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: PREPARE NUMERIC FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n[3/6] Preparing numeric features...\")\n",
    "\n",
    "# Handle unit_raw with SAME encoder from training\n",
    "def encode_unit_safe(unit):\n",
    "    \"\"\"Encode unit, return 0 for unknown units\"\"\"\n",
    "    if pd.isna(unit):\n",
    "        unit = 'unknown'\n",
    "    if unit in le_unit.classes_:\n",
    "        return le_unit.transform([unit])[0]\n",
    "    else:\n",
    "        # Unknown unit - use 0 or 'unknown' class\n",
    "        if 'unknown' in le_unit.classes_:\n",
    "            return le_unit.transform(['unknown'])[0]\n",
    "        return 0\n",
    "\n",
    "test_df['unit_encoded'] = test_df['unit_raw'].apply(encode_unit_safe)\n",
    "\n",
    "# Fill missing values same as training\n",
    "test_df['value_raw'] = test_df['value_raw'].fillna(0)\n",
    "test_df['pack_count'] = test_df['pack_count'].fillna(1)\n",
    "\n",
    "test_numeric_features = test_df[['value_raw', 'pack_count', 'unit_encoded']].values\n",
    "\n",
    "print(f\"✓ Numeric features: {test_numeric_features.shape}\")\n",
    "print(f\"  Unique units in test: {test_df['unit_raw'].nunique()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: CREATE TEST DATALOADER\n",
    "# ============================================================================\n",
    "print(\"\\n[4/6] Creating test dataloader...\")\n",
    "\n",
    "# Create test dataset (no target prices)\n",
    "test_dataset_inference = torch.utils.data.TensorDataset(\n",
    "    torch.FloatTensor(test_text_matrix),\n",
    "    torch.FloatTensor(test_title_matrix),\n",
    "    torch.FloatTensor(test_image_matrix),\n",
    "    torch.FloatTensor(test_numeric_features)\n",
    ")\n",
    "\n",
    "test_loader_inference = DataLoader(test_dataset_inference, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"✓ Test dataloader created\")\n",
    "print(f\"  Batches: {len(test_loader_inference)}\")\n",
    "print(f\"  Batch size: 128\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: LOAD TRAINED MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n[5/6] Loading trained model...\")\n",
    "\n",
    "# Initialize model with SAME architecture as training\n",
    "model = CrossAttentionFusion(\n",
    "    text_dim=test_text_matrix.shape[1],\n",
    "    title_dim=test_title_matrix.shape[1],\n",
    "    image_dim=test_image_matrix.shape[1],\n",
    "    numeric_dim=test_numeric_features.shape[1],\n",
    "    fusion_dim=256,\n",
    "    num_heads=8,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "# Load trained weights\n",
    "model.load_state_dict(torch.load('best_cross_attention_model.pt', map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "print(f\"✓ Model loaded from: best_cross_attention_model.pt\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: GENERATE PREDICTIONS\n",
    "# ============================================================================\n",
    "print(\"\\n[6/6] Generating predictions...\")\n",
    "\n",
    "test_predictions_log = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader_inference):\n",
    "        text, title, image, numeric = batch\n",
    "        \n",
    "        # Move to device\n",
    "        text = text.to(DEVICE)\n",
    "        title = title.to(DEVICE)\n",
    "        image = image.to(DEVICE)\n",
    "        numeric = numeric.to(DEVICE)\n",
    "        \n",
    "        # Predict\n",
    "        output = model(text, title, image, numeric)\n",
    "        test_predictions_log.append(output.cpu().numpy())\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  Processed {(i + 1) * 128}/{len(test_df)} samples...\")\n",
    "\n",
    "# Concatenate all predictions\n",
    "test_predictions_log = np.concatenate(test_predictions_log)\n",
    "\n",
    "# Convert from log space to original price scale\n",
    "test_predictions = np.expm1(test_predictions_log)\n",
    "\n",
    "print(f\"\\n✓ Predictions complete!\")\n",
    "print(f\"  Total predictions: {len(test_predictions)}\")\n",
    "print(f\"\\n  Prediction statistics:\")\n",
    "print(f\"    Min: ${test_predictions.min():.2f}\")\n",
    "print(f\"    Max: ${test_predictions.max():.2f}\")\n",
    "print(f\"    Mean: ${test_predictions.mean():.2f}\")\n",
    "print(f\"    Median: ${np.median(test_predictions):.2f}\")\n",
    "print(f\"    Std: ${test_predictions.std():.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: SAVE RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Add predictions to test dataframe\n",
    "test_df['price'] = test_predictions\n",
    "\n",
    "# Save full test data with predictions\n",
    "test_df.to_csv('test_with_predictions_cross_attention.csv', index=False)\n",
    "print(f\"\\n✓ Saved full results: test_with_predictions_cross_attention.csv\")\n",
    "\n",
    "# Create submission file (sample_id + price only)\n",
    "submission = test_df[['sample_id', 'price']].copy()\n",
    "submission.to_csv('submission_cross_attention.csv', index=False)\n",
    "print(f\"✓ Saved submission file: submission_cross_attention.csv\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(f\"\\nSample predictions (first 10):\")\n",
    "print(submission.head(10).to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INFERENCE COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n📊 Summary:\")\n",
    "print(f\"  Test samples: {len(test_df)}\")\n",
    "print(f\"  Predictions generated: {len(test_predictions)}\")\n",
    "print(f\"  Model used: best_cross_attention_model.pt\")\n",
    "print(f\"  Training SMAPE: 47.80%\")\n",
    "\n",
    "print(f\"\\n💾 Output files:\")\n",
    "print(f\"  - test_with_predictions_cross_attention.csv (full data)\")\n",
    "print(f\"  - submission_cross_attention.csv (for submission)\")\n",
    "\n",
    "print(f\"\\n✅ Ready to submit: submission_cross_attention.csv\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf039e8a-3126-47a8-9646-5ece0bd26ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
